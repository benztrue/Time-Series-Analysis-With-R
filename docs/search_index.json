[["index.html", "Time Series Analysis With R Chapter 1 Time Series Analysis With R 1.1 Objectives 1.2 Lectures 1.3 Home Work 1.4 Assessment 1.5 Syllabus and readings 1.6 Further information and support", " Time Series Analysis With R Nicola Righetti 2021-05-27 Chapter 1 Time Series Analysis With R This book will be updated as the course goes on. 1.1 Objectives This course is a practical introduction to time series analysis with R. It will introduce students to: The specificity of time series data; The free statistical software R to conduct time series analysis; Some of the main univariate and multivariate techniques to analyze time series data. At the end of the course, the students are expected to know the specificity of time series data and to be able to use R to perform simple time series analysis by applying the techniques described during the course. 1.2 Lectures 12 lectures (Thursday 11:30-13:00). Structure of the course: Theoretical concepts: this part of the course will introduce students to the main theoretical concepts of time series analysis; R Tutorial: this part of the course consists in a hands-on tutorial on the R functions necessary to perform time series analysis. Every part of a time series analysis project will be taken into account, including data wrangling, visual representation, and statistical analysis; Individual/Group work: this part of the course consists in individual and group work based on the application of the theoretical and practical knowledge described in the previous part of the course 1.3 Home Work Theoretical concepts can be studied, but you have to practice in order to learn R. 1.4 Assessment Assignments distributed during the course, dealing with demonstrating the understanding of key concepts (30%). A final data analysis project where participants will apply the knowledge and techniques learned during the course (70%). 1.5 Syllabus and readings This open book is specifically created for the 220050-1 SE SE Advanced Data Analysis 2 (2021S) course. It includes both theoretical concepts and the R tutorial with the necessary code to perform all the operations we are going to learn. The book also includes hyperlinks to additional free resources and readings. The mandatory readings will be listed in the Readings section of the book. A new part of the book will be uploaded online every weeks, following the program of the lessons. The link to this book is the following: Time-Series-Analysis-With-R. 1.6 Further information and support For any information, communication, or request for clarification, you can reach out to me at my University of Vienna address. "],["getting-started-with-r.html", "Chapter 2 Getting started with R 2.1 RStudio Interface and Data 2.2 Basic R", " Chapter 2 Getting started with R 2.1 RStudio Interface and Data 2.1.1 Download and Install RStudio This course is based on the statistical software R. R is easier to use in the development environment RStudio (it works on both Windows, Apple, and other OS). It is possible to download a free version of RStudio Desktop from the official websites. You might also use a free online version of RStudio by registering to the RStudio Cloud free plan. However, the free plan gives you just 15 hours per months. Our lessons take 4.5 hours per month, and since you also need to practice, the best choice is to install RStudio and R on your computer. Now we are going to see how to get started with RStudio Desktop. First, download and install a free version of RStudio Desktop and open the software. 2.1.2 Create a RStudio Project and Import data When starting a data analysis project with RStudio, we create a new dedicated environment where we will keep all the scripts (files containing the code to perform the analysis), data sets, and outputs of the analysis (such as plots and tables). This dedicated work-space is simply called a project. To create a new project with RStudio, follows these steps: click on File (on the top left); then, click on New Project; select New Directory, and New Project; choose a folder for the project, and give a name to your project. You can use the name Time-Series-Analysis-With-R; In this way, it will be created a new folder for the project, in the main folder specified in the previous step. In this folder, you will find a file .Rproj, the name of which is the name you assigned to your project. To work on this project, you just need to open the .Rproj file. 2.1.3 Create a Script Once the project has been created, we can open a new script and save it. A script is a file containing code. We can create a first script named basic-r-syntax, where you will test the basic code we are going to see. The script will be saved with extension .r. You can open, change, and save the file every time you work on it. To save your code is important, otherwise you would have to write the same code every time you work on the project! Create and save a script Update a script and run code 2.1.4 The RStudio User Interface The interface of RStudio is organized in four main quadrants: The top-left quadrant is the editor. Here you can create or open a script and compose the R commands. The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. The bottom-right quadrant is a window for graphics output, but it also has tabs to manage your file directories, R packages, and the R Help facility. On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the Run button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved). The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is the file tab, where you can navigate files and folders and find, for instance, the data sets you want to upload. The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There is also a tab for the R packages, and the R Help facility. 2.1.5 Load and Save Data To load data into R you can click on the file window in the top-right quadrant, navigate your files/folders, and once you have found your data set file, you can just click it and follow the semi-automated import procedure. Import Data Otherwise, you can upload a data set by using a function. For instance, to import a csv file, one of the most common format for data sets, it can be employed the function read.csv. The main argument of this function is the path of the file you want to upload. To specify the file path, consider that you are working within a specific environment, that is, your working directory is the folder of the project (you can double check the working directory you are working in, by running the command getwd()). Thus, to indicate the path of the data set you want to upload, you can write a dot followed by a slash ./, followed by the path of the data set inside the working directory. For instance, in the case below, the data set is saved in a folder named data inside the working directory. The name of the data set is tweets_vienna and its extension is .csv. Therefore, the code to upload the file is as follows: fake_news &lt;- read.csv(&quot;./data/fake-news-stories-over-time-20210111144200.csv&quot;) To save data there are a few options. Generally, if you want to save a data set, you can opt for the .csv or the .rds format. The .rds format is only readable by R, while the .csv is a “universal” format (you can read it with Excel, for instance). To save a file as .csv it can be used the function write.csv. The main arguments of this function are the name of the object that has to be saved, the path to the folder where the object will be saved, and the name we want to assign to the file. write.csv(fake_news, file = &quot;./data/fake_news.csv&quot;) To save .rds file the procedure is similar, but the saveRDS function has to be employed. Instead, to read an rds file, the appropriate function is readRDS. saveRDS(fake_news, file = &quot;./data/fake_news.rds&quot;) fake_news &lt;- readRDS(&quot;./data/fake_news.rds&quot;) # read a .rds file In the code above you can notice an hash mark sign followed by some text. It is a comment. Comments are textual content used to describe the code in order to make it easier to understand and reuse it. Comments are written after the hash mark sign (#), because the text written after the hash mark sign is ignored by R: you can read the comments, but R does not consider them as code. 2.1.6 Create new Folders It is a good practice to create, in the main folder of the project, sub-folders dedicated to different type of files used in the project, such as a folder “data” for the data sets. To create a new folder you can go to the Files windows in the RStudio interface, click New Folder, and give it a name. 2.2 Basic R 2.2.1 Objects An object is an R entity composed of a name and a value. The arrow (&lt;-) sign is used to create objects and assign a value to an object (or to change or “update” its previous value). Example: create an object with name “object_consisting_of_a_number” and value equal 2: object_consisting_of_a_number &lt;- 2 Enter the name of the object in the console and run the command: the value assigned to the object will be printed. object_consisting_of_a_number ## [1] 2 The object is equal to its value. Therefore, for instance, an object with a numerical value can be used to perform arithmetical operations. object_consisting_of_a_number * 10 ## [1] 20 The value of an object can be transformed: object_consisting_of_a_number &lt;- object_consisting_of_a_number * 10 object_consisting_of_a_number ## [1] 20 An object can also represent a function. Example: create an object for the sum (addition) function: function_sum &lt;- function(x, y){ result &lt;- x + y return(result) } The function can now be applied to two numerical values: function_sum(5, 2) ## [1] 7 Actually, we don’t need this function, since mathematical functions are already implemented in R. sum(5, 2) ## [1] 7 5 + 7 ## [1] 12 2 * 3 ## [1] 6 3^2 ## [1] 9 sqrt(9) ## [1] 3 The value of an object can be a number, a function, but also a vector. Vectors are sequences of values. vector_of_numbers &lt;- c(1,2,3,4,5,6,7,8,9,10) vector_of_numbers ## [1] 1 2 3 4 5 6 7 8 9 10 A vector of numbers can be the argument of mathematical operations. vector_of_numbers * 2 ## [1] 2 4 6 8 10 12 14 16 18 20 vector_of_numbers + 3 ## [1] 4 5 6 7 8 9 10 11 12 13 Other R objects are matrix, list, and data.frame. A matrix is a table composed of rows and columns containing only numerical values. a_matrix &lt;- matrix(data = 1:50, nrow = 10, ncol = 5) a_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 11 21 31 41 ## [2,] 2 12 22 32 42 ## [3,] 3 13 23 33 43 ## [4,] 4 14 24 34 44 ## [5,] 5 15 25 35 45 ## [6,] 6 16 26 36 46 ## [7,] 7 17 27 37 47 ## [8,] 8 18 28 38 48 ## [9,] 9 19 29 39 49 ## [10,] 10 20 30 40 50 A list is just a list of other objects. For instance, this list includes a numerical value, a vectors of numbers, and a matrix. a_list &lt;- list(object_consisting_of_a_number, vector_of_numbers, a_matrix) a_list ## [[1]] ## [1] 20 ## ## [[2]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[3]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 11 21 31 41 ## [2,] 2 12 22 32 42 ## [3,] 3 13 23 33 43 ## [4,] 4 14 24 34 44 ## [5,] 5 15 25 35 45 ## [6,] 6 16 26 36 46 ## [7,] 7 17 27 37 47 ## [8,] 8 18 28 38 48 ## [9,] 9 19 29 39 49 ## [10,] 10 20 30 40 50 A data.frame is like a matrix that can contain numbers but also other types of data, such as characters (a textual type of data), or factors (unordered categorical variables, such as gender, or ordered categories, such as low, medium, high). Data sets are usually stored in data.frame. For instance, if you import a csv or an Excel file in R, the corresponding R object is a data.frame. # this is an object (vector) consisting of a series of numerical values numerical_vector &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14) numerical_vector ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # this is another object (vector) consisting of a series of categorical values categorical_vector &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Monday&quot;, &quot;Wednesday&quot;,&quot;Thursday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;) categorical_vector ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Monday&quot; &quot;Tuesday&quot; &quot;Monday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Wednesday&quot; ## [9] &quot;Thursday&quot; &quot;Saturday&quot; &quot;Sunday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; # this is an object consisting of a data.frame, created combining vectors through the function &quot;data.frame&quot; a_dataframe &lt;- data.frame(&quot;first_variable&quot; = numerical_vector, &quot;second_variable&quot; = categorical_vector) a_dataframe ## first_variable second_variable ## 1 1 Monday ## 2 2 Tuesday ## 3 3 Monday ## 4 4 Tuesday ## 5 5 Monday ## 6 6 Wednesday ## 7 7 Thursday ## 8 8 Wednesday ## 9 9 Thursday ## 10 10 Saturday ## 11 11 Sunday ## 12 12 Friday ## 13 13 Saturday ## 14 14 Sunday To access a specific column of a data.frame, you can use the name of the data.frame, the dollar symbol $, and the name of the column. a_dataframe$first_variable ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 a_dataframe$second_variable ## [1] Monday Tuesday Monday Tuesday Monday Wednesday Thursday Wednesday Thursday ## [10] Saturday Sunday Friday Saturday Sunday ## Levels: Friday Monday Saturday Sunday Thursday Tuesday Wednesday It is possible to add columns to a data.frame by writing: the name of the data.frame the dollar sign a name for the new column the arrow sign &lt;- a vector of values to be stored in the new column (it has to have length equal to the other vectors composing the data.frame) a_dataframe$a_new_variable &lt;- c(12, 261, 45, 29, 54, 234, 45, 42, 6, 267, 87, 3, 12, 9) a_dataframe ## first_variable second_variable a_new_variable ## 1 1 Monday 12 ## 2 2 Tuesday 261 ## 3 3 Monday 45 ## 4 4 Tuesday 29 ## 5 5 Monday 54 ## 6 6 Wednesday 234 ## 7 7 Thursday 45 ## 8 8 Wednesday 42 ## 9 9 Thursday 6 ## 10 10 Saturday 267 ## 11 11 Sunday 87 ## 12 12 Friday 3 ## 13 13 Saturday 12 ## 14 14 Sunday 9 It is possible to visualize the first few rows of a data.frame by using the function head. head(a_dataframe) ## first_variable second_variable a_new_variable ## 1 1 Monday 12 ## 2 2 Tuesday 261 ## 3 3 Monday 45 ## 4 4 Tuesday 29 ## 5 5 Monday 54 ## 6 6 Wednesday 234 Exercise: visualize the first rows of a data.frame and access its columns 2.2.2 Functions A function is a coded operation that applies to an object (e.g.: a number, a textual feature etc.) to transform it based on specific rules. A function has a name (the name of the function) and some arguments. Among the arguments of a function there is always an object or a value, for instance a numerical value, which is the content the function is applied to, and other possible arguments (either mandatory or optional). Functions are operations applied to objects that give a certain output. E.g.: the arithmetical operation “addition” is a function that applies to two or more numbers to give, as its output, their sum. The arguments of the “sum” function are the numbers that are added together. The name of the function is written out of parentheses, and the arguments of the function inside the parentheses: sum(5, 3) ## [1] 8 Arguments of functions can be numbers but also textual features. For instance, the function paste creates a string composed of the strings that it takes as arguments. paste(&quot;the&quot;, &quot;cat&quot;, &quot;is&quot;, &quot;at&quot;, &quot;home&quot;) ## [1] &quot;the cat is at home&quot; In R you can sometimes find a “nested” syntax, which can be confusing. The best practice is to keep things as simple as possible. # this comment, written after the hash mark, describe what is going on here: two &quot;paste&quot; function nested together have been used (improperly! because they make the code more complicated than necessary) to show how functions can be nested together. It would have been better to use the &quot;paste&quot; function just one time! paste(paste(&quot;the&quot;, &quot;cat&quot;, &quot;is&quot;, &quot;at&quot;, &quot;home&quot;), &quot;and&quot;, &quot;sleeps&quot;, &quot;on&quot;, &quot;the&quot;, &quot;sofa&quot;) ## [1] &quot;the cat is at home and sleeps on the sofa&quot; To sum up, functions manipulate and transform objects. Data wrangling, data visualization, as well as data analysis, are performed through functions. 2.2.3 Data Types Variables can have different R formats, such as: double: numbers that include decimals (0.1, 5.676, 121.67). This format is appropriate for continuous variables; integer: such as 1, 2, 3, 10, 400. It is a format suitable to count data; factors: for categorical variables. Factors can be ordered (e.g.: level of agreement: “high”, “medium”, “low”), or not (e.g.: hair colors “blond”, “dark brown”, “brown”); characters: textual labels; logicals: the format of logical values (i.e.: TRUE and FALSE) dates: used to represent days; POSIX: a class of R format to represent dates and times. Figure 2.1: R data formats. Tables from Gaubatz, K. T. (2014). A Survivor’s Guide to R: An Introduction for the Uninitiated and the Unnerved. SAGE Publications. It is better to specify the appropriate type of data when importing a data set. In the example below, the data format are specified by using the import process of RStudio. Notice that the data of type “date” requires users to specify the additional information regarding the format of the dates. Indeed, dates can be written in many different ways, and to read dates in R it is necessary to specify the structure of the date. In the example, dates are in the format Year-Month-Day, which is represented in R as “%Y-%m-%d” (further details will be provided in another section of the book). Import data and specify data types 2.2.4 Excercise Upload the data set “election news small”, using the appropriate data format; Open the script “basic-r-script” and perform the following operations: Check the first few rows of the data set; Access the single columns; Save the data frame with the name “election_news_small_test” in the folder “data” by using the function “write.csv” (to review the procedure go to the section “Load and Save Data” on this book); Comment the code (the comments have to be written after the hash sign #); Save the script. "],["basic-data-wrangling-with-tidyverse.html", "Chapter 3 Basic Data Wrangling with Tidyverse 3.1 The Pipe Operator %&gt;% 3.2 Mutate 3.3 Rename 3.4 Summarize and group_by 3.5 Arrange 3.6 Filter 3.7 Select 3.8 Exercise", " Chapter 3 Basic Data Wrangling with Tidyverse Data wrangling is the process of transforming and mapping data from one “raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data. Another definition is as follows: Data wrangling is the process of profiling and transforming datasets to ensure they are actionable for a set of analysis tasks. One central goal is to make data usable: to put data in a form that can be parsed and manipulated by analysis tools. Another goal is to ensure that data is responsive to the intended analyses: that the data contain the necessary information, at an acceptable level of description and correctness, to support successful modeling and decision-making. How to “manipulate” data sets in R: use basic R functions; employ specific libraries such as tidyverse. Tidyverse is an R library composed of functions that allow users to perform basic and advanced data science operations. https://www.tidyverse.org. In R, a library (or “package”) is a coherent collection of functions, usually created for specific purposes. To work with the tidyverse library, it is necessary to install it first, by using the following command: install.packages(“tidyverse”). After having installed tidyverse (or any other library), it is necessary to load it, so as we can work with its functions in the current R session: # to load a library used the command library(NAME-OF-THE-LIBRARY) library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Besides using the function install.packages(NAME-OF-THE-LIBRARY) by using a line of code, it is also possible to use the RStudio interface. Install and Load a Library 3.1 The Pipe Operator %&gt;% Tidyverse has a peculiar syntax that makes use of the so-called pipe operator %&gt;%, like in the following example: a_dataframe %&gt;% group_by(second_variable) %&gt;% summarize(mean = mean(a_new_variable)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 7 x 2 ## second_variable mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Friday 3 ## 2 Monday 37 ## 3 Saturday 140. ## 4 Sunday 48 ## 5 Thursday 25.5 ## 6 Tuesday 145 ## 7 Wednesday 138 To manipulate data sets we can rely on the functions included in dplyr: a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges, such as mutate, rename, summarize. library(readr) tweets &lt;- read_csv(&quot;data/tweets_covid_small.csv&quot;, col_types = cols(created_at = col_datetime(format = &quot;%Y-%m-%d %H:%M:%S&quot;), retweet_count = col_integer())) head(tweets) ## # A tibble: 6 x 4 ## created_at screen_name source retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone 0 ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone 0 ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iPhone 3 ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App 0 ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App 4 ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iPad 96 Import data and specify data types (dates and times) 3.2 Mutate The function mutate adds new variables to a data.frame or overwrites existing variables. tweets &lt;- tweets %&gt;% mutate(log_retweet_count = log(retweet_count)) head(tweets) ## # A tibble: 6 x 5 ## created_at screen_name source retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone 0 -Inf ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone 0 -Inf ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iPhone 3 1.10 ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App 0 -Inf ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App 4 1.39 ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iPad 96 4.56 3.3 Rename Rename is a function to change the name of columns (sometimes it could be useful). tweets &lt;- tweets %&gt;% # rename (new_name = old_name) rename(device = source) head(tweets) ## # A tibble: 6 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone 0 -Inf ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone 0 -Inf ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iPhone 3 1.10 ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App 0 -Inf ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App 4 1.39 ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iPad 96 4.56 The previous two steps can be performed at the same time, by concatenating the operations through the pipe %&gt;% operator. # load again the data set library(readr) tweets &lt;- read_csv(&quot;data/tweets_covid_small.csv&quot;, col_types = cols(created_at = col_datetime(format = &quot;%Y-%m-%d %H:%M:%S&quot;), retweet_count = col_integer())) tweets &lt;- tweets %&gt;% mutate(log_retweet_count = log(retweet_count+1)) %&gt;% rename(device = source) head(tweets) ## # A tibble: 6 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone 0 0 ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone 0 0 ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iPhone 3 1.39 ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App 0 0 ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App 4 1.61 ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iPad 96 4.57 To check the data format of the variables stored in the data.frame, it can be used the command str(): str(tweets) ## tibble [100 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ created_at : POSIXct[1:100], format: &quot;2021-03-24 08:53:52&quot; &quot;2021-03-24 08:53:25&quot; &quot;2021-03-24 08:53:52&quot; ... ## $ screen_name : chr [1:100] &quot;DoYourThingUK&quot; &quot;DoYourThingUK&quot; &quot;AlexS1595&quot; &quot;MakesworthAcc&quot; ... ## $ device : chr [1:100] &quot;Twitter for iPhone&quot; &quot;Twitter for iPhone&quot; &quot;Twitter for iPhone&quot; &quot;Twitter Web App&quot; ... ## $ retweet_count : int [1:100] 0 0 3 0 4 96 0 1 0 3 ... ## $ log_retweet_count: num [1:100] 0 0 1.39 0 1.61 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. created_at = col_datetime(format = &quot;%Y-%m-%d %H:%M:%S&quot;), ## .. screen_name = col_character(), ## .. source = col_character(), ## .. retweet_count = col_integer() ## .. ) Sometimes variables are stored in the data.frame in the wrong format (see the paragraph “data type”), so we want to convert them into a new format. For this purpose we can use, again, the function mutate, along with other functions such as.integer, as.numeric, as.character, as.factors, or as.logical, as.Date, or as.POSIXct() based on the desired data format (it is possible and advisable to upload the data by paying attention to the type of data. If you upload the data in the right format, you can skip this step). tweets %&gt;% mutate(device = as.character(device)) %&gt;% head() ## # A tibble: 6 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone 0 0 ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone 0 0 ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iPhone 3 1.39 ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App 0 0 ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App 4 1.61 ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iPad 96 4.57 3.4 Summarize and group_by To aggregate data and calculate synthetic values (for instance, the average number of tweets by day), it can be used the function group_by (to aggregate data, for instance by day), and summarize, to calculate the summary values. tweets_summary &lt;- tweets %&gt;% group_by(screen_name) %&gt;% summarize(average_retweets = mean(retweet_count)) head(tweets_summary) ## # A tibble: 6 x 2 ## screen_name average_retweets ## &lt;chr&gt; &lt;dbl&gt; ## 1 2EXvoZ6nublpw1F 164 ## 2 AdilHaiderMD 80 ## 3 AlexS1595 3 ## 4 Andecave 20 ## 5 anshunandanpra4 2 ## 6 ApKido 150 It is also possible to create more than one summary variables at once. tweets_summary &lt;- tweets %&gt;% group_by(screen_name) %&gt;% summarize(average_retweets = mean(retweet_count), average_log_retweets = mean(log_retweet_count)) head(tweets_summary) ## # A tibble: 6 x 3 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2EXvoZ6nublpw1F 164 5.11 ## 2 AdilHaiderMD 80 4.39 ## 3 AlexS1595 3 1.39 ## 4 Andecave 20 3.04 ## 5 anshunandanpra4 2 1.10 ## 6 ApKido 150 5.02 3.4.1 Count occurrences A useful operation to perform when summarizing data, is to count the occurrences of a certain variable. For instance, to count the number of tweets sent by each user, it can be used the function n() inside the summarize function. tweets_summary &lt;- tweets %&gt;% group_by(screen_name) %&gt;% summarize(average_retweets = mean(retweet_count), average_log_retweets = mean(log_retweet_count), number_of_tweets = n()) head(tweets_summary) ## # A tibble: 6 x 4 ## screen_name average_retweets average_log_retweets number_of_tweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2EXvoZ6nublpw1F 164 5.11 1 ## 2 AdilHaiderMD 80 4.39 1 ## 3 AlexS1595 3 1.39 1 ## 4 Andecave 20 3.04 1 ## 5 anshunandanpra4 2 1.10 1 ## 6 ApKido 150 5.02 1 3.5 Arrange To explore a data set it can be useful to sort the data (e.g.: from the lowest to the highest value of a variable). With tidyverse, we can order a data.frame by using the function arrange. To sort the data from the highest to the lowest value (descending order) the minus sign (or the “desc” function) has to be added. tweets_summary %&gt;% arrange(-number_of_tweets) %&gt;% head() ## # A tibble: 6 x 4 ## screen_name average_retweets average_log_retweets number_of_tweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 iprdhzb 0.667 0.462 3 ## 2 benphillips76 3.5 1.45 2 ## 3 DoYourThingUK 0 0 2 ## 4 MakesworthAcc 2 0.805 2 ## 5 viralvideovlogs 3.5 1.45 2 ## 6 2EXvoZ6nublpw1F 164 5.11 1 tweets_summary %&gt;% arrange(desc(average_retweets)) %&gt;% head() ## # A tibble: 6 x 4 ## screen_name average_retweets average_log_retweets number_of_tweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Oliver_Miguel1 1988 7.60 1 ## 2 Lil_3arbiii 1627 7.40 1 ## 3 Kittyhawk681 1285 7.16 1 ## 4 JulesFox12 1091 7.00 1 ## 5 rosaesaa26 983 6.89 1 ## 6 lewisabzueta 822 6.71 1 Without the minus sign (or the “desc” command), data are sorted from the lowest to the highest value. tweets_summary %&gt;% arrange(number_of_tweets) %&gt;% head() ## # A tibble: 6 x 4 ## screen_name average_retweets average_log_retweets number_of_tweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2EXvoZ6nublpw1F 164 5.11 1 ## 2 AdilHaiderMD 80 4.39 1 ## 3 AlexS1595 3 1.39 1 ## 4 Andecave 20 3.04 1 ## 5 anshunandanpra4 2 1.10 1 ## 6 ApKido 150 5.02 1 3.6 Filter The function filter keeps only the cases (the “rows”) we want to focus on. The arguments of this function are the conditions that have to be fulfilled to filter the data: a) the name of the column that we want to filter, b) the values to be kept. tweets %&gt;% filter(retweet_count &gt;= 500) %&gt;% arrange(-retweet_count) ## # A tibble: 10 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for Android 1988 7.60 ## 2 2021-03-24 08:53:48 Lil_3arbiii Twitter for iPhone 1627 7.40 ## 3 2021-03-24 08:53:48 Kittyhawk681 Twitter Web App 1285 7.16 ## 4 2021-03-24 08:53:37 JulesFox12 Twitter for Android 1091 7.00 ## 5 2021-03-24 08:53:42 rosaesaa26 Twitter for Android 983 6.89 ## 6 2021-03-24 08:53:42 lewisabzueta Twitter for Android 822 6.71 ## 7 2021-03-24 08:53:42 jonvthvn08 Twitter for iPhone 768 6.65 ## 8 2021-03-24 08:53:34 florent61647053 Twitter for Android 768 6.65 ## 9 2021-03-24 08:53:37 Ritu89903967 Twitter for Android 709 6.57 ## 10 2021-03-24 08:53:33 Hurica3 Twitter for iPhone 575 6.36 In the examples below, notice the use of a double equal sign ==, and also of the quotation marks to indicate the modalities of a categorical variable. tweets %&gt;% filter(retweet_count == 1988) ## # A tibble: 1 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for Android 1988 7.60 tweets %&gt;% filter(device == &quot;Twitter for Android&quot;) ## # A tibble: 33 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:49 marcin_lukawski Twitter for Android 1 0.693 ## 2 2021-03-24 08:53:49 LebodyRanya Twitter for Android 0 0 ## 3 2021-03-24 08:53:47 anshunandanpra4 Twitter for Android 2 1.10 ## 4 2021-03-24 08:53:44 insoumise007 Twitter for Android 5 1.79 ## 5 2021-03-24 08:53:43 Metamorfopsies Twitter for Android 0 0 ## 6 2021-03-24 08:53:43 keepsmiling_130 Twitter for Android 164 5.11 ## 7 2021-03-24 08:53:43 lovebresil01 Twitter for Android 81 4.41 ## 8 2021-03-24 08:53:42 LightHealing Twitter for Android 1 0.693 ## 9 2021-03-24 08:53:42 lewisabzueta Twitter for Android 822 6.71 ## 10 2021-03-24 08:53:42 rosaesaa26 Twitter for Android 983 6.89 ## # … with 23 more rows It is also possible to use several conditions at the same time. tweets %&gt;% filter(device == &quot;Twitter for Android&quot;, retweet_count &gt; 200) %&gt;% arrange(-retweet_count) ## # A tibble: 8 x 5 ## created_at screen_name device retweet_count log_retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for Android 1988 7.60 ## 2 2021-03-24 08:53:37 JulesFox12 Twitter for Android 1091 7.00 ## 3 2021-03-24 08:53:42 rosaesaa26 Twitter for Android 983 6.89 ## 4 2021-03-24 08:53:42 lewisabzueta Twitter for Android 822 6.71 ## 5 2021-03-24 08:53:34 florent61647053 Twitter for Android 768 6.65 ## 6 2021-03-24 08:53:37 Ritu89903967 Twitter for Android 709 6.57 ## 7 2021-03-24 08:53:27 aspeaker66 Twitter for Android 331 5.81 ## 8 2021-03-24 08:53:42 JamesAn26254230 Twitter for Android 201 5.31 3.7 Select Select is used to keep just some of the columns of the original data.frame. For instance, we can apply the function in order to keep just the column “device” and “retweet_count”. tweets %&gt;% select(device, retweet_count) %&gt;% head() ## # A tibble: 6 x 2 ## device retweet_count ## &lt;chr&gt; &lt;int&gt; ## 1 Twitter for iPhone 0 ## 2 Twitter for iPhone 0 ## 3 Twitter for iPhone 3 ## 4 Twitter Web App 0 ## 5 Twitter Web App 4 ## 6 Twitter for iPad 96 3.8 Exercise Here are some exercises to consolidate the fundamental R skills learned during these first lessons: Download the csv file tweets_vienna_small.csv in this folder and put it into the project folder “data” Upload the data set in R, setting the appropriate formats for the variables Create a new script named “data-manipulation” with your surname and name as follows: “YOUR SURNAME-YOUR NAME-data-manipulation”, and save it In the script, write the code to perform the following operations: load the library “Tidyverse” show the first rows of the data frame by using the function head create a new data frame “tweets_vienna_small_updated” by updating the dataframe “tweets_vienna_small” by using the function mutate to create a new column “log_friends_count” whose values are the log of the values in the column “friends_count” (you don’t need to add 1 to the values in the column “friends_count”) save the updated dataframe “tweets_vienna_small_updated”, by using the following code to save a csv file (please change YOUR SURNAME-YOUR NAME with your actual surname and name): write.csv(tweets_vienna_small_updated, file = “./data/YOUR SURNAME-YOUR NAME-tweets_vienna_small_updated.csv”, row.names=F) (we add row.names=F to avoid saving the number that indexes each row) create a new data frame named “summary_tweets_vienna_small” aggregating the data by “screen_name” (using the function group_by) and then summarizing the data (by using the function summarize) as follows: in a column named “average_favorite_count”, calculate the average of “favorite_count” by “screen_name” (that is, by user) in a column named “average_retweet_count”, calculate the average of “retweet_count” (by user, obviously, since the data are already aggregated by user’ name) in a column named “number_of_tweets”, calculate the number of tweets published by each users (by using the function n()) save the “summary_tweets_vienna_small” in the data folder of the project, in csv format, and with the name “YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small.csv” (remember to specify row.names=F and to change YOUR SURNAME-YOUR NAME with your actual surname and name) create a new data frame object called “summary_tweets_vienna_small_filtered”, where you will save the data.frame summary_tweets_vienna_small, after having filtered the rows with average_retweet_count higher than 10 (by using the function filter), and after having selected the column “screen_name” and “average_retweet_count” (so, you should end up with a data frame with just two columns, “screen_name” and “average_retweet_count”, and the rows with “average_retweet_count” higher than 10) save the data frame “summary_tweets_vienna_small_filtered” with the name “YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small_filtered.csv” in the folder data (remember to specify row.names=F and to change YOUR SURNAME-YOUR NAME with your actual surname and name). Save the script “YOUR SURNAME-YOUR NAME-data-manipulation” with all the code you have used to perform these analysis. Write a comment in the script (using the hash mark #) if you are not able to do something. Upload the script “YOUR SURNAME-YOUR NAME-data-manipulation.r” and the files “YOUR SURNAME-YOUR NAME-tweets_vienna_small_updated.csv”, “YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small.csv”, and the file “YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small_filtered.csv” on Moodle, in the folder “HomeWork-1”. The deadline is Sunday 11 April. "],["basic-concepts.html", "Chapter 4 Basic Concepts 4.1 Time Series 4.2 Time Series Analysis 4.3 Stochastic and Deterministic Processes", " Chapter 4 Basic Concepts 4.1 Time Series A time series is a serially sequenced set of values representing a variable value at different points in time (VanLear, “Time Series Analysis”). It consists in measures collected through time, at regular time intervals, about an unit of observation, resulting in a set of ordered values. This regularity is the frequency of time series (which can be, for instance, hourly, weekly, monthly, quarterly, yearly etc.). Time series data are different from cross-sectional data, which are set of data observed on a sample of units taken at a given point in time, or where the time dimension is not relevant and can be ignored. Cross-sectional data are a snapshot of a population of interest at one particular point in time, while time series show the dynamical evolution of a variable over time. Panel data combine cross-sectional and time series data by observing the same units over time. Time is a fundamental variable in time series. It is often not relevant in other types of statistical analyses. Also from a sociological perspective (and psychological as well), we can see that past events influence future behaviors. Oftentimes, we can make reasonable prediction about future social behaviors just by observing past behaviors. Actually, social reproduction of behaviors over time and predictability of future social behaviors based on past experience and shared knowledge are essential to social order, and thus, a fundamental dimension of human society. From a statistical perspective, the impact of time resulting from repeated measurements over time on a single subject or unit, introduce a dependency among data points which prevents the use of some of the most common statistical techniques. In cross-sectional data, observations are assumed to be independent: values observed on one unit has no influence on values observed on other units. Time series observations have a different nature: a time series is not a collection of independent observations, or observations taken on independent units, but a collection of successive observations on the same unit. Observations are not taken across units at the same time (or without regards to time), but across time on the same unit. When dealing with time series data, time is an important factor to be taken into account. It introduces a new dimension to the data. For instance, we can calculate how a variable increases or decreases over time, if it peaks at a given moment in time, or at regular intervals. We consider not just if, and how much, a variable is correlated with another variable, but if there is a correlation over time among them, if the peaks in one variable precedes the peaks in the other one, or how much time it requires for a variable to have an impact on another one, and how much this impact changes over time. Importantly, when dealing with time series data, we have to to acknowledge that sampling adjacent points in time introduces a correlation in the data. This serial dependency creates correlated errors which violates the assumptions of many traditional statistical analyses and can bias the estimation of error for confidence intervals or significance tests. This characteristic of time series data, in general, precludes the use of common statistical approaches such as linear regression and correlation analysis, which assume the observations to be independent. The application of “standard” statistical techniques to time series data might lead to foolish, and totally unreliable results. For instance, the statisticians George Udny Yule wrote: «It is fairly familiar knowledge that we sometimes obtain between quantities varying with the time (time-variables) quite high correlations to which we cannot attach any physical significance whatever, although under the ordinary test the correlation would be held to be certainly “significant.” (…) the occurrence of such “nonsense-correlations” makes one mistrust the serious arguments that are sometimes put forward on the basis of correlations between time-series. […] When the successive x’s and y’s in a sample no longer form a random series, but a series in which successive terms are closely related to one another, the usual conceptions (of correlation, ed.) to which we are accustomed fail totally and entirely to apply» (Yule, G.U. (1926). Why do we sometimes get nonsense-correlations between Time-Series? A study in sampling and the nature of time-series. Journal of the royal statistical society, 89(1), 1-63.) A funny website reporting spurious time series correlation is tylervigen.com. Despite it can be funny to see these improbable correlations, we have to keep in mind that adopting the right approach to analyze data is a serious issue when doing research. In a paper on the American Journal of Political Science we can read, for instance: The results of the analysis below strongly suggest that the way event counts have been analyzed in hundreds of important political science studies have produced statistically and substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, and other problems result from the unknowing application of two common methods that are without theoretical justification or empirical utility in this type of data. Due to the peculiarity of time series data, time series analysis has been developed as a specific statistical methodology appropriate for the analysis of time-dependent data. Time series analysis aims at providing an understanding of the underlying processes and patterns of change over time of a unit of observation and the relations between variables observed over time, handling the time structure of the data in a proper way. 4.2 Time Series Analysis Time series analysis is an approach employed in many disciplines. Almost every field of study has data characterized by a time development, and every phenomenon with a temporal dimension can be conceived as a time series, and can be analyzed through time series analysis methods. Time series analysis are an important part of data analysis in disciplines such as economics, to analyze, for instance, inflation trends, marketing to analyze the number of clients of a store or number of accesses to an e-commerce website, in demography to study the growth of national population overtime or trends in population ageing, in engineering to analyze radio frequencies, in neurology to analyze brain waves detected through electroencephalograms. Political science can be interested in studying patterns in alternation of political parties in government, and digital communication can be interested in using time series analysis to study series of tweets using an hashtag, the news media coverage on a certain topic, or the trends in users searches on search engines, such as those provided by Google Trends. About the use of time series analysis in communication science, it can be observed that: “Many of the major theories and models in our field contain time as a central player: the two-step flow, cultivation, spiral-of-silence, agenda-setting, framing, and communication mediation models, to name a few (Nabi &amp; Oliver, 2009). Each articulates a set of processes that play out in time: Messages work their way through media systems and networks, citizens perceive the world around them and decide to communicate, or not, and they make choices about participation, presumably as a product of a process that includes communication exposure. Indeed, the words that animate our field—effect, flow, influence, dynamic, cycle—reveal our understanding of communication as a process, and processes have temporal dimensions (Box-Steffensmeier, Freeman, Hitt, &amp; Pevehouse, 2014). The perspective of time series analysis can help expand our notions of time’s role in these dynamics. We see several ways in which we can become more attentive to time in our field”. Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., &amp; Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13. “One of the most common applications of time series analyses in mass communication is in agenda-setting research. The approach is to correlate the national news coverage on a topic over time with public opinion or public policy on that topic, often to estimate lagged effects or the decay of effects over time. Likewise, both trends and cycles of television programming, viewing, and advertising, have been explored through time series analyses. In the interpersonal literature, the most popular and one of the most important applications of time series analysis has been the investigation of mutual adaptation in the form of patterns of reciprocity or compensation between conversational partners over the course of an interaction.” (C. Arthur VanLear, “Time Series Analysis”, in Allen, M. (Ed.). (2017). The SAGE encyclopedia of communication research methods. Sage Publications). In general, we can distinguish at least the following objectives of a time series analysis study: DESCRIPTION: Description of a process characterized by an intrinsic temporal dimension. Simple examples of related questions are: is there an upward trend? Is there a peak at a certain point in time? Is there a regular pattern recurring every year, in a particular moment in time? Descriptive questions like these can be answered via descriptive time series analysis. EVALUATION: Evaluation of the impact of a certain event, occurring in a particular point in time, on a process. For instance: did a change in social media moderation policy, such as those that led to ban accounts linked to conspiracy theories, impacted on the quantity of fake news shared online by users? Specific time series techniques can be used to perform this kind of analysis. EXPLANATION: Explanation of a phenomenon characterized by a time series structure on the basis of related variables. For instance: does the quantity of news shared on Facebook help explaining the polarization of the debate online? Does the volume of news media articles on a topic help explaining the growth of the debate online on the same topic? Inferential statistical techniques, such as regression models developed for time series, are used to answer questions like these. FORECASTING: Prediction of the future values of a process. For instance: can we expect that news media coverage on a certain topic keep growing in the near future? This is the subject of time series forecasting. We can also distinguish between univariate and multivariate time series analysis. Time series analysis can be used to explain the temporal dependencies within and between processes. By temporal dependency within a social process, we mean that the current value of a variable is, in part, a function of previous values of that same variable. To analyze univariate structure of time series, univariate techniques are used. Temporal dependency between social processes, conversely, indicates that the current value of a variable is in part a function of the previous values of other variables. Multivariate time series analysis are used to explain the relations between time series. 4.3 Stochastic and Deterministic Processes A general distinction can be made between time series, based on their deterministic or non-deterministic nature. A deterministic time series is one which can be explicitly expressed by an analytic expression. It has no random or probabilistic parts. It is always possible to exactly predict its future behavior, and state how it behaved in the past. Deterministic processes are pretty rare when dealing with individual and social behaviors! Predicting future behaviors of a crowd, of a person, of a social group, can be reasonably possible, sometimes, based on past behaviors and other contextual information, since human behavior is partly influenced by the past. However, it is not totally determined by the past. There is always a certain degree of uncertainty in the prediction; human behaviors are, generally speaking, not fully predictable. Social and individual behaviors, therefore, are non-deterministic. A non-deterministic time series cannot be fully described by an analytic expression. It has some random, or probabilistic component, that prevents its behavior from being explicitly described. It could be possible to say, in probabilistic terms, what its future behavior might be. However, there is always a residual, unpredictable, component. A time series may be considered non-deterministic also because all the information necessary to describe it explicitly is not available, although it might be in principle, or because the nature of the generating process, or part of it, is inherently random. We can say that the time series analyzed in social science have always, at least, a stochastic component that makes them not totally deterministic. Since non-deterministic time series have a random component, they follow probabilistic rather than deterministic laws. Random data are not defined by explicit mathematical relations, but rather in statistical terms, that is, by probability distributions and parameters such as mean and variance. Non-deterministic time series can be analyzed by assuming that they are manifestations of probabilistic or stochastic processes. "],["time-series-objects.html", "Chapter 5 Time Series Objects 5.1 Time Series Objects 5.2 Exercise", " Chapter 5 Time Series Objects 5.1 Time Series Objects Every object we manipulate in R is characterized by a specific structure. Objects’ structures vary depending on the type of object: a list, a matrix, or a data.frame, are different objects with different structures. Every structure has its own manipulation methods. For instance, it can be accessed and analyzed by using different functions and strings of code. In R there are many different types of object. To get an overview you can refer to this handbook, to the R manual, or the chapter 5 of this free online book. In this course we are going to learn more about the data structures we have to deal with when conducting time series analysis in R, that is, the structure of time series objects and data sets. Time series data sets in R can be represented by different objects. Specific libraries (coherent collections of functions) can give different structures to time series data sets. There are many R libraries for handling and working with time series objects. Some of them are more general and other are useful to perform very specific analysis. On this page you can find a comprehensive list of the R libraries for time series analysis. For now, we just need to know that different libraries can create time series objects with different structures which can be manipulated through different functions. This means that not all the objects can be analyzed with all the functions, as well that there is not always compatibility between R libraries. Many functions have been developed with reference to specific libraries and objects, or require a particular object structure. As a consequence, creating a time series object with a certain structure or a certain library can imply that we can use some functions and perform only a certain type of analysis. In other words, specific type of objects could introduce specific constrains to data analysis (and visualization), so it could be wise to plan in advance the necessary analyses, so as to select the necessary libraries and data structure. We now introduce three types of objects that are commonly used to store and analyze time series data: The data.frame (in base R) The ts object (in base R) The xts object (created through the library xts). We analyze the structures of these objects and their strengths and limitations. In the next chapter, we’ll also learn the methods available to visualize them. 5.1.1 Time Series as Data Frames Data frames (data.frame) are the most common data set structure in R. A data.frame is simply a table cases by variables (each row is a case and each column is a variable). To see an example of data.frame containing time series data, we can upload a data set containing the number of news articles mentioning the keyword “elections” published by USA news media. I retrieved this data set from MediaCloud, a free and open source platform for studying media ecosystem that tracks millions of stories published online. You can download the data set at this link. We can upload the .csv file by using the function read.csv. The main argument of the read.csv function is the path of the file. elections_news &lt;- read.csv(&quot;./data/elections-stories-over-time-20210111144254.csv&quot;) By using the function class we can see that this is a data.frame. class(elections_news) ## [1] &quot;data.frame&quot; We can check the first few rows of the data.frame by using the function head, which shows the first few rows of the data set, so as to get an idea of the structure of this simple data.frame. head(elections_news) ## date count total_count ratio ## 1 2015-01-01 373 25611 0.01456405 ## 2 2015-01-02 387 31932 0.01211950 ## 3 2015-01-03 289 24646 0.01172604 ## 4 2015-01-04 322 25513 0.01262102 ## 5 2015-01-05 567 39982 0.01418138 ## 6 2015-01-06 626 42366 0.01477600 This data.frame contains time series data: the first column contains dates, and the other columns contain the values of the observations. We can also see that the data.frame seems to contain daily data, where each row corresponds to a specific day. The data frame also includes, in the column “count”, the number of news articles mentioning the keyword “elections”, in the column “total_count”, the total number of news articles on all the topics, and in the column “ratio” the proportion of news articles mentioning the keyword (count/total_count). The function head (and tail) can be impractical with data.frame including a lot of columns, so it could be better to use the function str to check the structure of the data.frame. str(elections_news) ## &#39;data.frame&#39;: 2192 obs. of 4 variables: ## $ date : Factor w/ 2192 levels &quot;2015-01-01&quot;,&quot;2015-01-02&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ count : int 373 387 289 322 567 626 507 521 531 346 ... ## $ total_count: int 25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ... ## $ ratio : num 0.0146 0.0121 0.0117 0.0126 0.0142 ... As you can see in the output of the function str, the format of the column date is Factor. The format is, in this case, automatically attributed by R, but (as we have already said) it can be specified before importing the data. Factor is an appropriate format for categorical variables, but R includes a specific format for dates and times. In this case we have just a date, so we can convert it to a variable of type date. We can change the format of the variable by using the function as.Date. elections_news$date &lt;- as.Date(elections_news$date) str(elections_news) ## &#39;data.frame&#39;: 2192 obs. of 4 variables: ## $ date : Date, format: &quot;2015-01-01&quot; &quot;2015-01-02&quot; &quot;2015-01-03&quot; ... ## $ count : int 373 387 289 322 567 626 507 521 531 346 ... ## $ total_count: int 25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ... ## $ ratio : num 0.0146 0.0121 0.0117 0.0126 0.0142 ... We can also perform the same operation with tidyverse, by using the function mutate. library(tidyverse) elections_news &lt;- elections_news %&gt;% mutate(date = as.Date(date)) A data.frame is the common format for data sets, including time series data sets. We can do many things with data stored in this format, such as creating plots and performing various types of analysis. However, to handle time series in R there are more specific data formats. 5.1.2 Time Series as TS objects The basic object created to handle time series in R is the object of class ts. The name stands for “Time Series”. An example of ts object is already present in R under the name of “AirPassengers”, a time series data set in ts format. We can load this data set with the function data. data(AirPassengers) By applying the function class we can see that this is an object of class ts. class(AirPassengers) ## [1] &quot;ts&quot; AirPassengers is a small data set so we can print all the data set to see its structure, which is an example of the standard structure of a ts object. AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 By calling the str function we get synthetic information on the object. str(AirPassengers) ## Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ... The AirPassengers data set is a univariate time series representing monthly totals of international airline passengers from 1949 to 1960. As every time series, it has a start date and an end date. It also has a frequency, which is the frequency at which the observations were taken. All this characteristics differentiate a ts object from a data.frame. The structure of a data.frame lacks the start and the end date, and the frequency value. The functions start, end, and frequency, can be applied to a ts object to check their values. We started by saying that some functions work with some objects but not with other types of objects. This is an example. These functions, indeed, work with ts objects just because they are part of their structures, and are arguments usually specified when this kind of object is created. They do not work if applied to a data.frame object, since a data.frame structure does not include the start and end date, nor the frequency of observations. It can be seen that the ts structure is much more specific for time series data. start(AirPassengers)[1] ## [1] 1949 end(AirPassengers)[1] ## [1] 1960 frequency(AirPassengers)[1] ## [1] 12 Importantly, the frequency of a time series is assumed to be regular over time. This applies to time series in general, and not just to ts objects. In this case, the time series starts on January 1949 and ends on December 1969, and has monthly frequency. Monthly frequency is indicated in ts as “12”, meaning 12 months. Indeed, the reference unit of a ts object is a year. So, quarterly data, for instance, have frequency equal to 4. To create a ts object is necessary to follow specific steps and use specific functions. To exemplify the process of creation of a ts object we take the example of the data contained in the AirPassengers data set, and store them in a data.frame (you don’t need to learn how to do that, just copy and paste the code). A data.frame is the data set format you will probably start with, so it can be useful to see how to create a ts object starting from a data.frame. date &lt;- seq.Date(from = as.Date(&quot;1949-01-01&quot;), to = as.Date(&quot;1960-12-01&quot;), by=&quot;month&quot;) passengers &lt;- as.vector(AirPassengers) data_frame_format &lt;- data.frame(&quot;Date&quot; = date, &quot;Passengers&quot; = passengers) To create a “ts” time series object starting from a data.frame, we need: To specify which column contains the observations. In this case, the column name is “Passengers”. We then need to specify the start and end date, which in this case are in the format year/month, but can be just years in case of yearly data. The ts format for the start/end date is the following: c=(YEAR, MONTH). The c represents the concatenate function, and it concatenates the year and the month in a single vector. Finally, we indicate the frequency of the time series observations. The frequency is specified based on the time period of a year, so in this case we have a frequency equal to 12, because we have monthly observation, meaning that we have 12 observation per year. ts_format &lt;- ts(data = data_frame_format$Passengers, start=c(1949, 01), end=c(1960, 12), frequency = 12) ts_format ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 class(ts_format) ## [1] &quot;ts&quot; It’s important to notice that yearly, quarterly, and monthly data work fine with the ts structure, but more fine grained data create complications and are not totally suitable for a ts structure. This is due to the fact that time series objects require the frequency of observations to be regular and in ts the observations have to be regular with reference to a year. Unfortunately, a time series that spans over many years cannot be composed of a constant number of days, since the number of days will be sometimes 365 and other time 366, in case of leap years. This is a limitation of ts objects. However, when dealing with monthly data or data with frequency lower than one month (such as quarterly data), ts works great. 5.1.3 Time Series as XTS/ZOO objects Time series can be stored in object of class xts/zoo. This class of objects is created with the library xts, which is related and an extension of the package zoo (another package to deal with time series data). As other libraries, it requires to be installed and loaded. # install.packages(&quot;xts&quot;) library(xts) The xts object is more flexible that the ts one. We can create an xts time series by starting from the data.frame we have just created. Similarly to what required by ts, we need to specify: the column of the data.frame (or the vector) containing the data; the column of the data.frame (or the vector) containing the dates/times (which has to be in a date/time format); the frequency of observations. We can use the data.frame already created with the AirPassengers data to create a new xts object. xts_format &lt;- xts(x = data_frame_format$Passengers, order.by = data_frame_format$Date, frequency = 12) class(xts_format) ## [1] &quot;xts&quot; &quot;zoo&quot; Also the structure of this object, like the ts one, includes the range of dates of the time series, with it starting and ending date. str(xts_format) ## An &#39;xts&#39; object on 1949-01-01/1960-12-01 containing: ## Data: num [1:144, 1] 112 118 132 129 121 135 148 148 136 119 ... ## Indexed by objects of class: [Date] TZ: UTC ## xts Attributes: ## NULL head(xts_format) ## [,1] ## 1949-01-01 112 ## 1949-02-01 118 ## 1949-03-01 132 ## 1949-04-01 129 ## 1949-05-01 121 ## 1949-06-01 135 5.2 Exercise Download the data set Austrian_Local_Media, a data set including metrics from Facebook pages of Austrian Local Media (monthly observations from January 2015 to December 2020 on quantity of posts and interactions) Set the “beginning_of_interval” to the appropriate “Date” format Create a ts object, called “ts_object_1”, using as data values the post_count values Create a xts object, called “xts_object_1”, using as data values the post_count values Use the Tidyverse function “mutate” to create a new column with the number of interaction by post (total_interactions/post_count) and call it “interaction_ratio” Create a ts object, called “ts_object_2”, using as data values the interaction_ratio values Create a xts object, called “xts_object_2”, using as data values the interaction_ratio values Try to plot these objects, by using the command plot.ts(ts_object_1), plot.ts(ts_object_2), plot.xts(xts_object_1), plot.xts(xts_object_2) "],["plot-time-series.html", "Chapter 6 Plot Time Series 6.1 Plot Time Series Objects 6.2 plot.ts 6.3 plot.xts 6.4 ggplot", " Chapter 6 Plot Time Series 6.1 Plot Time Series Objects In this lecture we are going to learn how to plot time series data. We will take into account three main functions: ggplot from the tidyverse library, plot.ts from base R, and plot.xts from the xts library. Ggplot is probably the most versatile function from the perspective of the graphical results that can be obtained, but also the more complex, while for ordinary visualization, plot.ts is probably the easiest tool. Plotting time series is an important part of the analysis because it permits to visualize and explore the data, both from a univariate perspective (focusing on the characteristics of a single time series) and from a multivariate perspective (focusing on the characteristics of many time series, and on the relations between them). To visualize and explore the relations between time series, we’ll learn to plot a single time series as well as many different time series at once. 6.2 plot.ts You can visualize a time series by using the function plot.ts() applied to a time series data in a ts format. An example of ts time series is provided by the AirPassengers dataset, already included in R (you can load the data by running data(“AirPassengers”)). data(&quot;AirPassengers&quot;) plot.ts(AirPassengers) You can add many details to your plot, such as a title, a label for the y axis and for the x axis, change the colors of the plot (use colors() to see the list of standard colors in R), and the size of the line. plot.ts(AirPassengers, main = &quot;PASSENGERS&quot;, xlab = &quot;1949-1960 (monthly data)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;violetred3&quot;, lwd=5) You can use plot.ts to plot two (or more) time series together, a useful operation to take a look at their relations. The different time series must have the same structure (the same starting date, the same ending date, and the same frequency), and should be stored in the same ts object. To merge in one “ts” object two or more time series already in the ts format, you can employ the function ts.union(). You can plot both the time series in the same plot, or create two different plots, by using the option “plot.type” and specifying single or multiple. With lwd you control the line width (or the line size): Line types can either be specified as an integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) or as one of the character strings “blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, or “twodash”, where “blank” uses ‘invisible lines’ (i.e., does not draw them). With lty you control the line type: The line width, a positive number, defaulting to 1. The interpretation is device-specific, and some devices do not implement line widths less than one. # create a &quot;toy&quot; time series with the same lenght of the AirPassenger one AirPassengers_2 &lt;- AirPassengers + 100 AirPassengers_3 &lt;- AirPassengers + 300 AirPassengers_multi &lt;- ts.union(AirPassengers, AirPassengers_2, AirPassengers_3) plot.ts(AirPassengers_multi, main = &quot;Three time series&quot;, xlab = &quot;TIME&quot;, ylab = &quot;VALUES&quot;, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;black&quot;), lwd=c(1, 1, 1), lty=c(1, 2, 3), plot.type = &quot;single&quot;) plot.ts(AirPassengers_multi, main = &quot;Three time series&quot;, xlab = &quot;TIME&quot;, ylab = &quot;VALUES&quot;, col = &quot;blue&quot;, lwd=4, plot.type = &quot;multiple&quot;) With the parameter nc you can control the number of columns used to display the data. plot.ts(AirPassengers_multi, main = &quot;Three time series&quot;, xlab = &quot;TIME&quot;, ylab = &quot;VALUES&quot;, col = &quot;orange&quot;, lwd=4, plot.type = &quot;multiple&quot;, nc=3) To explore long time series it can be useful to focus on a limited time window. To do that, you can subset the data by using the function window. It is a function which extracts the subset of data observed between the specified start and end time. You can use the function window in the plot.ts function (alternatively, you can create a new object by applying the function window first, and then plot the new object). plot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954, 12)), main = &quot;PASSENGERS (1950-1955)&quot;, xlab = &quot;1950-1955 (monthly data)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;violetred3&quot;, lwd=5) In the window function, you can also specify a frequency, and the series is then re-sampled at the new frequency. For instance, by re-sampling at quarterly frequency, the function keeps the observations made on January, April, July, and October, and by re-sampling at a six-month frequency, it keeps the observations made on January and July. plot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954,12), frequency = 4), main = &quot;PASSENGERS (1950-1955) - QUARTERLY DATA&quot;, xlab = &quot;1950-1955 (Quarterly data)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;violetred3&quot;, lwd=5) You can use the window function also with more than one time series. plot.ts(window(AirPassengers_multi, start=c(1950, 01), end=c(1954,12), frequency = 4), main = &quot;PASSENGERS (1950-1955) - QUARTERLY DATA&quot;, xlab = &quot;1950-1955 (Quarterly data)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;violetred3&quot;, lwd=5) To learn something more about the graphical options of plot.ts, you can open and read the help page by using ?plot.ts. The question mark followed by the name of a function opens the help page of that function. 6.3 plot.xts To plot a xts object we can similarly use the plot.xts function. You can create a xts object with the xts function (see the previous chapter), but if you already have a ts object, you can also convert it to a xts object by using the function as.xts AirPassengers_xts &lt;- as.xts(AirPassengers) plot.xts(AirPassengers_xts, main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;steelblue2&quot;, lwd=5) By using multi.panel=TRUE, or multi.panel=FALSE you can plot all the time series in the same panel or using different panels. AirPassengers_multi_xts &lt;- as.xts(AirPassengers_multi) plot.xts(AirPassengers_multi_xts, main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;black&quot;), multi.panel = T) plot.xts(AirPassengers_multi_xts, main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;black&quot;), multi.panel = F) To subset the data, in order to visualize and focus on just one part of the series, instead of the function window, you have to write the dates into squared brackets as in the examples below. plot.xts(AirPassengers_multi_xts[&quot;1950-01/1954-12&quot;], main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;black&quot;), multi.panel = F) plot.xts(AirPassengers_xts[&quot;1950-01/1956-06&quot;], main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;black&quot;), multi.panel = F) You can also change the frequency of the observations by using specific functions in the xts library. By using the function periodicity you can find the frequency of the time series. periodicity(AirPassengers_xts) ## Monthly periodicity from Jan 1949 to Dec 1960 With the function to.period you can re-sample the data to “seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, “quarters”, and “years”. You can only re-sample the data from a higher to a lower frequency, but not from a lower to a higher one. For instance, if you have monthly data, you can aggregate the data in quarterly or yearly data, but you cannot create a weekly or hourly time series. The result of the to.period function will contain the open (first) and close (last) value for the given period, as well as the maximum and minimum over the new period, reflected in the new high and low, respectively. to.period(AirPassengers_xts, period=&quot;years&quot;) ## AirPassengers_xts.Open AirPassengers_xts.High AirPassengers_xts.Low AirPassengers_xts.Close ## Dec 1949 112 148 104 118 ## Dec 1950 115 170 114 140 ## Dec 1951 145 199 145 166 ## Dec 1952 171 242 171 194 ## Dec 1953 196 272 180 201 ## Dec 1954 204 302 188 229 ## Dec 1955 242 364 233 278 ## Dec 1956 284 413 271 306 ## Dec 1957 315 467 301 336 ## Dec 1958 340 505 310 337 ## Dec 1959 360 559 342 405 ## Dec 1960 417 622 390 432 You can plot all the values, or select a value by using the square brackets with a comma, followed by the number of the column you want to plot (see the table above, with 4 columns). This notation is a way to access the columns (and the rows) of a data.frame or matrix. You write the name of the data.frame or the matrix, and the squared brackets indicate the index of the rows, in the first position before the comma, and the index of the columns, in the second position, after the comma. So, for instance, to access the value in the second column and the second row of the data.set “data”, you can write data[2,2], and to access the values in the third column and first row, you can write data[1,3]. If you leave a blank space in the column or row space, you get all the values in that column or rows. Therefore, by writing data[,2], or data[,3], you get all the values in the column 2 and 3, respectively. plot.xts(to.period(AirPassengers_xts, period=&quot;years&quot;)[,2], main = &quot;PASSENGERS&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;black&quot;), multi.panel = F) You can also re-sample and calculate the average (or another statistics) for the new period. For instance, in the example we re-sample the data by year and calculate the average. It is also possible to calculate other statistics such as, for instance, the median, just by writing “median” instead of “mean”. index_years &lt;- endpoints(AirPassengers_xts, on = &quot;year&quot;) AirPassengers_xts_year_avg &lt;- period.apply(AirPassengers_xts, INDEX=index_years, FUN=mean) plot.xts(AirPassengers_xts_year_avg, main = &quot;PASSENGERS (Year Average)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, lwd=5, lty=1, col = &quot;blue&quot;, multi.panel = F) You can find additional details on xts and plot.xts by reading the help functions. At this link you can find a synthetic presentation of the functions of the xts library. 6.4 ggplot Ggplot2 is the tidyverse library for data visualization. We can use it to create time series plots and many other types of plot. We upload a data set, first, and we set the appropriate time format for the date. We place our data set (“elections_news”) inside the ggplot function. Notice that the ggplot syntax is similar to the tidyverse one, but uses the plus sign instead of the pipe one (%&gt;%). To create a line plot with ggplot it is necessary to use the geom_line function. This function requires two parameters: the data for the x-axis and the data for the y-axis. These parameters have to be written inside the aes function. You can also specify the colors and the size of the line. By using additional function, after the plus sign, you can also set the labels for the x- and y-axes, and the title, the subtitle, and the caption of the plot. You can also change the overall aspect of the plot by using one of the themes included in the library. ggplot(elections_news) + geom_line(aes(x = date, y = ratio), color = &quot;snow4&quot;, size = 0.5) + ylab(&quot;News Articles&quot;) + xlab(&quot;Date&quot;) + labs(title = &quot;Time Series of News Articles on Elections&quot;, subtitle = &quot;Data from MediaCloud&quot;, caption = &quot;Data Analysis II&quot;) + theme_classic() You can also plot more than one series. For instance, you can create two plots, and then use the function grid.arrange, from the library gridExtra to combine the plots together. # install.packages(&quot;gridExtra&quot;) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine p1 &lt;- elections_news %&gt;% ggplot() + geom_line(aes(x = date, y = ratio), col = &quot;black&quot;, size = 0.5) + ylab(&quot;News Articles (ratio)&quot;) + xlab(&quot;Date&quot;) + ggtitle(&quot;MediaCloud Data on Elections (Daily)&quot;) p2 &lt;- elections_news %&gt;% ggplot() + geom_line(aes(x = date, y = count), col=&quot;red&quot;, size=0.5) + ylab(&quot;News Articles (count)&quot;) + xlab(&quot;Date&quot;) + ggtitle(&quot;MediaCloud Data on Elections (Daily)&quot;) grid.arrange(p1,p2) You can also plot two or more than two series in the same plot. elections_news &lt;- elections_news %&gt;% mutate(time_series_data_2 = count*2, time_series_data_3 = count*4) ggplot(elections_news) + geom_line(aes(x = date, y = count), col = &quot;black&quot;, size = 0.5) + geom_line(aes(x = date, y = time_series_data_2), col = &quot;blue&quot;, size = 0.5) + geom_line(aes(x = date, y = time_series_data_3), col=&quot;red&quot;, size=0.5) + ylab(&quot;&quot;) + xlab(&quot;Date&quot;) + ggtitle(&quot;MediaCloud Data on Elections (Daily)&quot;) To focus on a shorter time window, we can use the dplyr function filter. Besides filtering the data, we add a function “scale_x_datetime”, which control the labels on the x-axis, specifying we want to use monthly labels. elections_news %&gt;% filter(date &gt;= &quot;2016-01-01&quot; &amp; date &lt; &quot;2017-01-01&quot;) %&gt;% ggplot() + geom_line(aes(x = date, y = count), col = &quot;black&quot;, size = 0.5) + scale_x_date(breaks=&quot;month&quot;, date_labels =&quot;%Y-%m&quot;) + theme(axis.text.x = element_text(angle = 45, hjust=1)) + ylab(&quot;News Articles (Ratio)&quot;) + xlab(&quot;Day&quot;) + ggtitle(&quot;MediaCloud Data on Elections (Monthly) - 2016&quot;) You can also use ggplot to annotate the date. To create annotations in ggplot you can use the function “annotate”, to label the data point, and “geom_segment”, to trace lines for connecting data points to labels. You can learn more about annotation in ggplot here: https://ggplot2-book.org/annotations.html And about lines here: https://ggplot2.tidyverse.org/reference/geom_segment.html ggplot(elections_news) + geom_line(aes(x = date, y = count), col = &quot;grey50&quot;, size = 0.25) + ylim(c(0, 15000)) + # 1 EVENT annotate(&quot;label&quot;, x = as.Date(&quot;2018-11-01&quot;), y = 14500, label = &quot;Midterm Elections\\nNovember 2018&quot;, color = &quot;white&quot;, fill=&quot;orange&quot;, fontface=&quot;bold&quot;, size=3) + # add a line. You can also use an arrow by adding in geom_segment: # arrow = line(length = unit(0.2, &quot;cm&quot;), ends = &quot;last&quot;) geom_segment(aes(x = as.Date(&quot;2018-11-01&quot;), xend = as.Date(&quot;2018-11-01&quot;), y = 0, yend = 14500), color = &quot;orange&quot;, size = 0.2, linetype = 1) + # 2 EVENT annotate(&quot;label&quot;, x = as.Date(&quot;2019-05-01&quot;), y = 12000, label = &quot;Pennsylvania Elections\\nMay 2019&quot;, color = &quot;white&quot;, fill=&quot;orange&quot;, fontface=&quot;bold&quot;, size=3) + geom_segment(aes(x = as.Date(&quot;2019-05-01&quot;), xend = as.Date(&quot;2019-05-01&quot;), y = 0, yend = 12000), color = &quot;orange&quot;, size = 0.2, linetype = 1) + # 3 EVENT annotate(&quot;label&quot;, x = as.Date(&quot;2016-11-01&quot;), y = 12000, label = &quot;Presidential Elections\\nNovember 2016&quot;, color = &quot;white&quot;, fill=&quot;orange&quot;, fontface=&quot;bold&quot;, size=3) + geom_segment(aes(x = as.Date(&quot;2016-11-01&quot;), xend = as.Date(&quot;2016-11-01&quot;), y = 0, yend = 12000), color = &quot;orange&quot;, size = 0.2, linetype = 1) + # 4 EVENT annotate(&quot;label&quot;, x = as.Date(&quot;2020-11-01&quot;), y = 12000, label = &quot;Presidential\\nElections\\nNovember\\n2020&quot;, color = &quot;white&quot;, fill=&quot;orange&quot;, fontface=&quot;bold&quot;, size=3) + geom_segment(aes(x = as.Date(&quot;2020-11-01&quot;), xend = as.Date(&quot;2020-11-01&quot;), y = 0, yend = 12000), color = &quot;orange&quot;, size = 0.2, linetype = 1) + ylab(&quot;News Articles&quot;) + xlab(&quot;Date&quot;) + labs(title = &quot;MediaCloud Data on Elections&quot;, subtitle = &quot;Peaks annotated with relevant political events&quot;, caption = &quot;Advanced Data Analysis University of Vienna&quot;) + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(face = &quot;italic&quot;)) + theme_gray() "],["structural-decomposition.html", "Chapter 7 Structural Decomposition 7.1 Components of a time series 7.2 Structural decomposition 7.3 Adjust time series 7.4 White Noise and Stationarity", " Chapter 7 Structural Decomposition 7.1 Components of a time series A time series can be considered composed of 4 main parts: trend, cycle, seasonality, and the irregular or remainder/residual part. 7.1.1 Trend and Cycle The Trend component is the longest-term behavior of a time series. The simplest model for a trend is a linear increase or decrease, but the trend has not to be linear. In the AirPassengers time series there is a clear upward, linear trend. library(xts) data(&quot;AirPassengers&quot;) AirPassengers_xts &lt;- as.xts(AirPassengers) plot.xts(AirPassengers_xts) 7.1.1.1 Stochastic and Deterministic Trend There is a distinction between deterministic and stochastic trends. A deterministic trend is a fixed function of time. If a series has a deterministic trend, the increase (or decrease) in the value of the series is a function of time. For instance, it may appear to grow or decline steadily over time. A deterministic trend can be linear, as well as non linear. Deterministic trends have plausible explanations (for example, a deterministic increasing trend in the data may be related to an increasing population). A series with deterministic trend is also called trend stationary. A stochastic trend wanders up and down or shows change of direction at unpredictable times. Time series with a stochastic trend are also said to be difference stationary. An example of stochastic trend is provided by the so-called random walk process. Random Walk is a particular time series process in which the current values are combinations of the previous ones (\\(x_t = x_{t-1} + w_t\\), where \\(x_{t-1}\\) is the value immediately before \\(x\\), and \\(w_t\\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (stochastic trend). Starting from the same initial point, the same process can generate different time series. set.seed(111) Random_Walk &lt;- arima.sim(n = 500, model = list(order = c(0,1,0))) plot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = &quot;Random Walk&quot;) set.seed(555) Random_Walk &lt;- arima.sim(n = 500, model = list(order = c(0,1,0))) plot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = &quot;Random Walk&quot;) A paper on The effectiveness of social distancing in containing Covid-19 shows an example of stochastic trend and complex, deterministic nonlinear trends represented by polynomials. Figure 7.1: Figure 5 shows the actual number of Covid-19 cases recorded in the UK up to 17 June 2020. The stochastic trend estimated earlier is superimposed on the actual observations and so are two deterministic nonlinear trends represented by polynomials of degrees 5 and 6. We can see that the stochastic trend captures the slow growth at the beginning of the sample period whereas the two deterministic trends do not. The stochastic trend is better also at capturing the sharp increase represented by observation number 72.. (original caption) The trend component of the series is often considered along with the cyclic one (trend-cycle). The cyclical component is represented by fluctuations (rises and falls) not occurring at a fixed frequency. The cycle component is therefore different from the seasonal variation (see below) in that it does not follow a fixed calendar frequency. 7.1.2 Seasonality The Seasonal component is a repeated pattern occurring at a fixed time period such as the time of the year or the day of the week (the frequency of seasonality, which is always a fixed and known frequency). There is a clear seasonal variation in the AirPassenger time series: bookings were highest during the summer months of June, July, and August and lowest during the autumn/winter months. plot.xts(AirPassengers_xts[&quot;1954-01/1955-12&quot;]) It is possible to plot the distributions of data by months by using the function boxplot and cycle, to visualize the increasing number of passengers during the summer months. In this case, cycle is used to refer to the positions of each observation in the (yearly, in this case) cycle of observations (every year is considered to be a cycle of 12 observations). boxplot(AirPassengers ~ cycle(AirPassengers)) The library forecast, an R package that provides methods and tools for displaying and analysing time series forecasts, includes a function to create a “polar” seasonal plot. # install.packages(&quot;forecast&quot;) library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## This is forecast 8.13 ## Want to meet other forecasters? Join the International Institute of Forecasters: ## http://forecasters.org/ ggseasonplot(AirPassengers, polar=TRUE) An example of weekly seasonality can be found in the COVID-19 statistics. Figure 7.2: Covid statistics (Google) Cyclic and seasonal variations can look similar. Both cyclic and seasonal variations have ‘peak-and-trough’ patterns. The main difference is that in seasonal patterns the period between successive peaks (or troughs) is constant, while in cyclical patterns the distance between successive peaks is irregular. 7.1.3 Residuals The irregular or remainder/residual component is the random-like part of the series. In general, when we fit mathematical models to time series data, the residual error series represents the discrepancies between the fitted values, calculated from the model, and the data. A good model encapsulates most of the deterministic features of the time series, and the residual error series should therefore appear to be a realization of independent random variables from some probability distribution. The analysis of residuals is thus important to judge the fit of a model. In this case, its residual error series appears to be a realization of independent random variables. Often the random variable is conceived as a Gaussian random variable. We’ll return to these topics in the last part of the chapter. AirPassengers_Random &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;)$random par(mfrow = c(2,1)) plot(AirPassengers_Random, xlab=&quot;&quot;, ylab=&quot;&quot;) hist(na.omit(AirPassengers_Random), main = &quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;) 7.2 Structural decomposition Along with the analysis of the peaks (see previous chapter), analyzing a time series based on these structural parts can be an important exploratory step. It helps understanding the likely causes of the series features and formulate an appropriate time series model. For instance, in the case of the AirPassengers series, we could hypothesize that the increasing trend is due to the rising prosperity in the aftermath of the Second World War, greater availability of aircraft, cheaper flights due to competition between airlines, and an increasing population. The seasonal variation, instead, seems to coincide with vacation periods. Decomposition methods try to identify and separate the above mentioned parts of a time series. Usually they consider together the trend and cycle (trend-cycle) - the longer-term changes in the series - and the seasonal factors - periodic fluctuations of constant length happening at a specific calendar frequency). There are two main ways through which these elements can be combined together: in the additive and the multiplicative form: The additive model (\\(x_{t} = m_{t} + s_{t} + z_{t}\\), where \\(x_{t}\\) is the observed time series, \\(m_{t}\\) is the trend-cycle component, \\(s_{t}\\) is the seasonal component and \\(z_{t}\\) is the residual) is useful when the seasonal variation is relatively constant over time The multiplicative model (\\(x_{t} = m_{t} * s_{t} * z_{t}\\)) is useful when the the seasonal effects tends to increase as the trend increases. There are different methods to decompose a time series. Here we consider the function decompose. This function is defined as Classical Seasonal Decomposition by Moving Averages. The function decompose uses a moving average (MA) approach to filter the data. Moving average is a classical approach to extract the trend from a time series by averaging out the seasonal effects. 7.2.1 Moving Average Moving average is a process that replaces each value \\(x_{t}\\) with an average of its current value \\(x_{t}\\) and its immediate neighbors in the past and future. For instance, it is possible to calculate a simple moving average by using the closest neighbors of a point, as follows: \\(x_{t} = \\frac{1}{3} (x_{t-1} + x_{t} + x_{t+1})\\). This is called Centered Moving Average. The number of neighbors in the past and future is determined by the analyst and is also called width of the window. The time window for the moving average is chosen by considering the frequency of the data and their seasonal effects. For instance, monthly data, which are supposed to show monthly seasonality (for instance, in the AirPassengers data there are more passengers during the summer months), can be averaged by using a period of 12 months (six months before and after each point. Since we have an even number of months, some other calculation are necessary. For instance, the moving average value for July, is calculated by averaging the average of January up to December, and the average of February up to January. R functions do this for you). The centered moving average is an example of a smoothing procedure that is applied retrospectively to a time series with the objective of identifying an underlying signal or trend. Smoothing procedures usually use points before and after the time at which the smoothed estimate is to be calculated. A consequence is that the smoothed series will have some points missing at the beginning and the end unless the smoothing algorithm is adapted for the end points. In the case of monthly data, for instance, the moving average filter determines the lost of the first and last six months of data. Smoothing procedures like moving average, allows the main underlying trend to emerge by filtering out seasonality and noise, so they are used to get an idea of the long-term underlying process of a time series. elections_news &lt;- read_csv(&quot;data/elections-stories-over-time-20210111144254.csv&quot;, col_types = cols(date = col_date(format = &quot;%Y-%m-%d&quot;))) en &lt;- as.xts(x = elections_news$ratio, order.by = elections_news$date) en2 &lt;- rollmean(en, k = 2) en4 &lt;- rollmean(en, k = 4) en8 &lt;- rollmean(en, k = 8) en16 &lt;- rollmean(en, k = 16) en32 &lt;- rollmean(en, k = 32) enALL &lt;- merge.xts(en, en2, en4, en8, en16, en32) # notice the NA elements increasing as the width of the moving average increase head(enALL, 10) ## en en2 en4 en8 en16 en32 ## 2015-01-01 0.01456405 0.01334178 NA NA NA NA ## 2015-01-02 0.01211950 0.01192277 0.01275765 NA NA NA ## 2015-01-03 0.01172604 0.01217353 0.01266199 NA NA NA ## 2015-01-04 0.01262102 0.01340120 0.01332611 0.01285129 NA NA ## 2015-01-05 0.01418138 0.01447869 0.01320110 0.01253790 NA NA ## 2015-01-06 0.01477600 0.01300100 0.01294493 0.01250958 NA NA ## 2015-01-07 0.01122600 0.01141117 0.01241382 0.01267144 NA NA ## 2015-01-08 0.01159633 0.01182664 0.01169304 0.01285992 0.01257820 NA ## 2015-01-09 0.01205695 0.01197492 0.01214178 0.01262867 0.01239793 NA ## 2015-01-10 0.01189290 0.01245691 0.01277492 0.01226887 0.01234396 NA plot.xts(enALL[&quot;2015-01-01/2016-01-01&quot;], multi.panel = T) 7.2.2 Decompose To apply the function decompose, we need a ts object. Considering the AirPassengers time series, since the seasonal effect tends to increase as the trend increases, we can use a multiplicative model. AirPassengers_dec &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) plot(AirPassengers_dec) As an example of additive model we can use data from the “Seatbels” data set. data(&quot;Seatbelts&quot;) seatbelts &lt;- Seatbelts[,5] plot.ts(seatbelts) seatbelts_dec &lt;- decompose(seatbelts, type=&quot;additive&quot;) plot(seatbelts_dec) The residual part of the model should be (approximately) random, which indicates that the model explained (most of) the significant patterns in the data (the “signal”), leaving out the “noise”. par(mfrow = c(1,2)) plot.ts(seatbelts_dec$random, main=&quot;Residuals&quot;, ylab=&quot;&quot;) hist(seatbelts_dec$random, breaks = 25, freq = F, main = &quot;Histogram&quot;) lines(density(seatbelts_dec$random, na.rm = T), col=&quot;red&quot;) We can re-create the original time series starting from its elements (we don’t actually need to do that, it is just for illustrative purposes). par(mfrow=c(2,1)) plot(AirPassengers_dec$trend * AirPassengers_dec$seasonal * AirPassengers_dec$random, xlim=c(1950, 1960), ylim=c(0,600), main = &quot;&#39;Re-composed&#39; series&quot;, ylab=&quot;&quot;) plot(AirPassengers, xlim=c(1950, 1960),ylim=c(0,600), main = &quot;Original series&quot;, ylab=&quot;&quot;) 7.3 Adjust time series Sometimes the analyst is not interested in the trend or in the seasonal variation in the data, and might want to remove them, in order to let other underlying process to emerge more clearly. Other times, some components of time series can be misleading, leading to inflated or spurious correlations, and can be preferable to remove them before proceding with the analysis. 7.3.1 Seasonal adjusted data It is common to find seasonally adjusted data, that is time series from which the seasonal component has been removed. This happen quite often in economics, for instance (but in other disciplines as well), where certain growing trends can be considered trivial, and explained based on solid theory. Other parts of the series are instead considered more important, and removing the seasonal component allow them to emerge more clearly, as summarized by Granger, C. W. (1978), Seasonality: causation, interpretation, and implications. In Seasonal analysis of economic time series: Presumably, the seasonal is treated in this fashion, because it is economically uninportant, being dull, superficially easily explained, and easy to forecast but, at the same time, being statistically important in that it is a major contributor to the total variance of many series. The presence of the seasonal could be said to obscure movements in other components of greater economic significance. (…) It can be certainly be stated that, when considering the level of an economic variable, the low frequency components (the trend-cycle, ed.) are usually both statistically and economically important. (…) Because of their dual importance, it is desirable to view this component as clearly as possible and, thus, the interference from the season should be removed. (…) the preference for seasonally adjusted data is so that they can more clearly see the position of local trends or the place on the business cycle. It is certainly true that for any series containing a strong season, it is very difficult to observe these local trends without seasonal adjustment. Moreover, seasonality can lead to spurious correlations: (…) if the relationship between a pair of economic variables is to be analyzed, it is obviously possible to obtain a spurious relationship if the two series contain important seasonals. By adjusting series, one possible source of spurious relationship is removed. However, adjust for seasonality a series is application specific, and sometimes this part of the series can be of interest: Firms having seasonal fluctuations in demand for their products, for example, may need to make decisions based largely on the seasonal component (…) and a local government may try to partially control seasonal fluctuations in unemployment. (…) Only by having both the the adjusted and the unadjusted data available can these potential users gain the maximum benefit from all of the effort that goes into collecting the information. There are many different methods to adjust data for seasonality. A simple approach is based on the results of the decomposition process, and consists in substracting (in the case of an addittive decomposition model) the seasonal component from the original series, or dividing the original series by the seasonal component (in the case of a multiplicative model). data(&quot;Seatbelts&quot;) seatbelts &lt;- Seatbelts[,5] seatbelts_dec &lt;- decompose(seatbelts, type=&quot;additive&quot;) seatbelts_deseason &lt;- seatbelts - seatbelts_dec$seasonal seat &lt;- ts.intersect(seatbelts, seatbelts_deseason) plot.ts(seat, plot.type = &quot;single&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), main = &quot;Original (red) and Seasonally Adjusted Series (blue)&quot;) Adjust for seasonal variations makes it possible to observe potentially noteworthy fluctuations. In the case of the AirPassengers data, for instance, the seasonally adjusted plot shows more clearly an anomaly in the year 1960 that was not noticeable in the raw data. AirPassengers_decompose &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) AirPassengers_seasonal &lt;- AirPassengers_decompose$seasonal AirPassengers_deseasonal &lt;- (AirPassengers/AirPassengers_seasonal) AirPass &lt;- ts.intersect(AirPassengers, AirPassengers_deseasonal) plot.ts(AirPass, plot.type = &quot;single&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), lty = c(3,1), main = &quot;Original (red) and Seasonally Adjusted Series (blue)&quot;) Below you can find another example with data from social media (you can download them here art1, and art2), consisting in posts published by pages of news media. art1 &lt;- read_csv(&quot;data/art1.csv&quot;) art2 &lt;- read_csv(&quot;data/art2.csv&quot;) art1_summary &lt;- art1 %&gt;% mutate(post_created_date = as.Date(post_created_date)) %&gt;% complete(post_created_date = seq.Date(min(post_created_date), max(post_created_date), by = &quot;day&quot;)) %&gt;% group_by(post_created_date) %&gt;% summarize(posts = n()) art2_summary &lt;- art2 %&gt;% mutate(post_created_date = as.Date(post_created_date)) %&gt;% complete(post_created_date = seq.Date(min(post_created_date), max(post_created_date), by = &quot;day&quot;)) %&gt;% group_by(post_created_date) %&gt;% summarize(posts = n()) art1_ts &lt;- ts(data = art1_summary$posts, frequency = 7) art2_ts &lt;- ts(data = art2_summary$posts, frequency = 7) art &lt;- ts.intersect(art1_ts, art2_ts) art1_dec &lt;- decompose(art1_ts) art2_dec &lt;- decompose(art2_ts) art1_seas &lt;- art1_dec$seasonal art2_seas &lt;- art2_dec$seasonal art1_deseas &lt;- art1_ts - art1_seas art2_deseas &lt;- art2_ts - art2_seas art_des &lt;- ts.intersect(art1_deseas, art2_deseas) art_all &lt;- ts.intersect(art, art_des) plot.ts(art_all, plot.type = &quot;single&quot;, col = c(&quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(3,3,1,1), main = &quot;Original (red) and Seasonally Adjusted Series (blue)&quot;) By calculating a simple correlation between the original series and the de-seasonalized series, it can be observed that the correlation coefficients changes. cor(art)[1,2] ## [1] 0.9641655 cor(art_des)[1,2] ## [1] 0.7209823 7.3.2 Detrended series As the series can be asjusted for seasonality, they can be also adjusted for trend based on the same reasons. A similar process can also be used to remove the trend from the data, in particular in the case of a deterministic trend. In the case of a stochastic trend, instead, the usual practice is to detrend the data through differencing. Differecing means taking the first difference of consecutive points in time \\(x_t\\) - \\(x_{t-1}\\). In this way, the resulting series represents the relative change from one point in time to another. Random_Walk &lt;- arima.sim(n = 500, model = list(order = c(0,1,0))) Random_Walk_diff &lt;- diff(Random_Walk) plot.ts(Random_Walk, main = &quot;Random Walk&quot;, col = &quot;blue&quot;, ylab=&quot;&quot;) plot.ts(Random_Walk_diff, main = &quot;Differenced Random Walk&quot;, col = &quot;blue&quot;, ylab=&quot;&quot;) Detrending a time series can be important before applying some statistical techniques, for instance before calculating the correlation between two time series. Time series with a trend component can reveal spurious correlations, since correlations may exist just because two variables are trending up or down at the same time. By detrending the time series, it can be more appropriately measured if the change in one time series over time is related to the change in another time series. Detrending time series is also used when researchers consider irrelevant the trend. This is the case when the trend is considered an obvious characteristic of the process. For instance, economists can take for granted that there is an increasing trend in GDP due to inflation, and thus they may want to “clean” the data to eliminate this trivial trend. They are more interested in deviations from the growth, than in the growth that they consider a “normal” characteristic of the process. There are also statistical tests to ascertain the presence of a trend. A monotonic trend can be detected with the Mann–Kendall trend test. The null hypothesis is that the data come from a population with independent realizations and are identically distributed. For the two sided test, the alternative hypothesis is that the data follow a monotonic trend (read the help: ?mk.test). The function to calculate this test is included in the package “trend”. # install.packages(&quot;trend&quot;) library(trend) mk.test(AirPassengers, alternative = &quot;greater&quot;) ## ## Mann-Kendall trend test ## ## data: AirPassengers ## z = 14.382, n = 144, p-value &lt; 2.2e-16 ## alternative hypothesis: true S is greater than 0 ## sample estimates: ## S varS tau ## 8.327000e+03 3.351643e+05 8.098232e-01 7.4 White Noise and Stationarity We said that the residual part of the model should be (approximately) random, which indicates that the model explained most of the significant patterns in the data (the “signal”), leaving out the “noise”. The standard model of independent random variation in time series analysis is known as white noise (a term coined in an article published in Nature in 1922, where it was used to refer to series that contained all frequencies in equal proportions, analogous to white light). The charts below show how a white noise process looks like. When we introduced the concept of deterministic and stochastic trend, we said that the series showing the first type of trend are also called trend stationary, and the series showing the second type of trend are also called difference stationary. Both these names refer to the concept of stationarity. A process is stationary if it is homogeneous, that is, if it has no distinguished points in times or, in other words, its statistical qualities are the same for any point in time. There are more or less stringent definition of stationarity (strict and weak stationarity), and the most used for practical purposes is the so-called weak-stationarity. In this sense, a time series is said to be stationary if there is: no trend (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations); no change in variance over time (time invariant variance); no auto-correlation (we’ll return to this topic in the next chapters) White noise is an example of stationary time series. As you can see in the chart above, white noise time series is pretty regular, the mean is always the same (0) and there are no changes in variance over time: the plot looks much the same at any point in time! Also through differencing, the series can achive a stationary form. This is the case, in particular, of the series with a stochastic trend, that are also called difference-stationary, exactly because through differencing they become stationary. plot.ts(Random_Walk_diff, main = &quot;Differenced Random Walk&quot;, col = &quot;blue&quot;, ylab=&quot;&quot;) The process through which stationarity is reached is also called Pre-Whitening, and it can be used as a pre-processing phase before conducting correlation and regression analysis: Once the form(s) of serial dependency that best account for the series are identified, they are removed from the series. This is called prewhitening and is used to produce a series that is a “white noise” process (i.e., a process that is free of serial dependency with each value statistically independent of other values in the series). Once pre-whitening is accomplished the values of that series can be correlated with, used to predict, or predicted from, the values in other contemporaneous time series (usually also pre-whitened) representing other variables of interest. By removing serial dependency, the pre-whitening process makes these analyses free of correlated errors. It also removes the possibility that a common temporal trend or pattern is a confounding explanation for the observed association between the two variable series (VanLear, “Time Series Analysis”) The logic behind the process and the importance of white noise is also well explained in these sentences: This “residual” part of the data, indeed, can be used as a dependent variable, giving the analyst confidence that any time series properties in the data will not account for any observed correlation between the covariates and the dependent variable. In the univariate context, the white noise process is important because it is what we would like to “recover” from our data – after stripping away the ways in which a univariate series can essentially explain itself. By removing the time series properties of our data, leaving only white noise, we have a series that can then be explained by other sources of variation. Another way to conceptualize white noise is as the exogenous portion of the data-generating process. Each of our data series is a function of those forces that cause the series to rise or fall (the independent variables we normally include to test our hypotheses) and of time series properties that lead those forces to be more or less “sticky”. After we filter away those time series properties, we are left with the forces driving the data higher or lower. We often refer to these forces as shocks – and these shocks then reverberate in our data, sometimes for a short spell or a long spell or even infinitely. The goal of time series analysis is to separately model the time series properties (the reverberations) so the shocks (i.e., the white noise) can be captured. (Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., &amp; Pevehouse, J. C. (2014). Time series analysis for the social sciences. Cambridge University Press.) "],["correlations-and-arima.html", "Chapter 8 Correlations and ARIMA 8.1 Auto-Correlation (ACF and PACF) 8.2 ARIMA models 8.3 Cross-correlation 8.4 Examples in literature", " Chapter 8 Correlations and ARIMA 8.1 Auto-Correlation (ACF and PACF) In the previous chapter we said that a time series is said to be stationary if there is: no trend (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations); no change in variance over time (time invariant variance); no auto-correlation (we’ll return to this topic in the next chapters) Auto-correlation or serial correlation is an important characteristic of time series data and can be defined as the correlation of a variable with itself at different time points. Autocorrelation has many consequences. It prevents us to use traditional statistical methods such as linear regression, which assume that the observations are independent from each other. In presence of autocorrelation, the estimated standard errors of the parameter estimates will tend to be less than their true value. This will lead to erroneously high statistical significance being attributed to statistical tests (the p values will be smaller than they should be). In this section we introduce an important tool for the diagnosis of the properties of a time series, including autocorrelation: the correlogram. The accurate study of correlogram is a common step in many time series analysis procedures. 8.1.1 Correlogram: ACF and PACF The correlogram is a chart that presents one of two statistics: the autocorrelation function (ACF).The ACF statistic measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) where k is the number of lead periods into the future. It measures the correlation between any two points based on a given interval. It is not strictly equivalent to the Pearson product moment correlation. In R, ACF is calculated and visualized with the function “acf”; the partial autocorrelation function (PACF). The PACF(k) is a measure of correlation between times series observations that are k units apart, after the correlation at intermediate lags has been controlled for or “partialed” out. In other words, the PACF measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) after it has stripped out the effect of the intermediate x’s. In R, the PACF is calculated and visualized with the function “pacf”. It is useful to detect correlations that are not evident in ACF. Let’s consider, as an example, the correlogram of a random walk process. We know that this is a particular time series process in which the current values are combinations of the previous ones (\\(x_t = x_{t-1} + w_t\\), where \\(x_{t-1}\\) is the value immediately before x, and \\(w_t\\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (stochastic trend). The ACF of a random walk time series, indeed, shows a correlation between values in the series: even values not so close are notwithstanding correlated. Random_Walk &lt;- arima.sim(n = 499, model = list(order = c(0,1,0))) acf(Random_Walk) Instead, the PACF, which removes the correlations between intermediate values, shows a correlation at lag 1, that is, it shows that the overall correlation depends on consequent values. The dotted blue lines signal the boundaries of statistical significance. pacf(Random_Walk) We can clearly visualize the auto-correlation by using a simple scatterplot, by plotting two consecutive lines of points. plot(Random_Walk[1:499], Random_Walk[2:500]) The ACF or PACF of a white noise process is very different. We know that white noise is a stationary process, without distinguishable points in time and no correlation between points. Indeed, the ACF of white noise shows no correlation (the only line above statistical significance is at zero, which is nothing to be worried about, since it just means that each point is correlated with itself). White_Noise &lt;- arima.sim(n = 500, model = list(order = c(0,0,0))) acf(White_Noise) In the PACF we can see that there is nothing above the dotted line (which means that there is nothing statistically significant). pacf(White_Noise) If we plot two consecutive lists of points by using a scatterplot, we can see there is no serial correlation (no pattern is visible): plot(White_Noise[1:499], White_Noise[2:500]) 8.2 ARIMA models The ACF and PACF plots can be used to diagnose the main characteristics of a time series and find a proper statistical model. We talk about univariate models, since they are models to describe a single time series. Univariate time series can be modeled as Auto Regressive (AR), Integrated (I), and Moving Average (MA) processes. These models are synthesized using the acronym ARIMA. When a seasonal (S) component is also taken into account, we also use the acronym SARIMA. 8.2.1 Auto Regressive (AR) models We just said that a time series is often characterized by auto-correlation, so we can clearly deduce that we can model it by using a regression model, that is, by regressing the time series on its past values. In this way we have an auto-regressive model: a regression of \\(x_{t}\\) on past terms \\(x_{t-k}\\) from the same series. In time series analysis, past terms \\(x_{t-k}\\) from a same series are called lags. The lagged values of a time series are its delayed values, where the delay can be of an arbitrary amount of time \\(k\\). For instance, considering a simple series of 4 data points distributed from time \\({t+0}\\) (first data point) to time \\({t+3}\\) (last data point) \\({x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}\\), the corresponding lagged series, assuming \\(k=1\\), is \\({NA, x_{t+0}, x_{t+1}, x_{t+2}}\\). Notice that the first data point is missing since there is no data point behind it, and the other data points are shifted one time point ahead. An auto-regressive (AR) model can be described as follows (the \\(\\alpha\\) are coefficients, \\(t\\) are time points, \\(w\\) is a random component or white noise): \\[ {x_t} = \\alpha x_{t-1} + \\alpha x_{t-2} + \\alpha x_{t-k} + {w_t} \\] AR_1 &lt;- arima.sim(n = 500, list(order = c(1,0,0), ar = 0.90)) plot(AR_1, main = &quot;AR(1)&quot;) The ACF of an autoregressive process typically shows a slow and gradual decay in autocorrelation over time. acf(AR_1) The PACF of an autoregressive process shows a peak in correspondence with the order of the model. In the case of an AR(1) the peak is at time 1. pacf(AR_1) In the case of an AR(3) the peak is at time 1, 2, and 3. AR_3 &lt;- arima.sim(n = 500, list(order = c(3,0,0), ar = c(0.3, 0.3, 0.3))) plot(AR_3, main = &quot;AR(3)&quot;) pacf(AR_3) Now we can see that the random walk process we have seen above is a particular case of auto-regressive model. In a random walk process, each value is the previous one plus a random part: \\[ x_t = x_{t-1} + w_t \\] Thus each point \\(x_t\\) is correlated with the previous one \\(x_{t-k}\\) where the lag value \\(k\\) is equal to 1 (\\({k=1}\\)). Therefore, a random walk process is an auto-regressive model of order 1, since just 1 lag is taken into consideration in the auto-regressive model (and with \\(\\alpha = 1\\)). The order of an auto-regressive model is indicated by parenthesis, e.g.: AR(1). The AR process can have different characteristics (and different ACF and PACF) based on the parameters. 8.2.2 Moving Average (MA) models We already know Moving Average as a method to smooth time series and detect a trend. When referring to Moving Average as a process (MA), we refer to a process in which the values of the series are a function of a weighted average of past errors. In other terms, a moving average (MA) process is a linear combination of the current white noise term and the \\(q\\) most recent past white noise terms: \\[ {x_t} = w_t + \\beta w_{t-1} + ... + \\beta w_{t-q} \\] The order of a MA process indicates the lags of white noise taken into account in the model (e.g: MA(3)). MA_3 &lt;- arima.sim(n = 500, list(order = c(0,0,3), ma = c(0.3, 0.3, 0.3))) plot(MA_3, main = &quot;MA(3)&quot;) The ACF plot of a MA process shows a more clear cut-off after the term corresponding to the order ot the process. It is different from the ACF of an AR process, which shows a more gradual decay. acf(MA_3) The PACF of a MA process shows an up-and-down movement and does not shut off, but instead tapers toward 0 in some manner. pacf(MA_3) 8.2.3 Integrated (I) process An integrated process is a non-stationary time series process that becomes stationary when transformed by differencing. In other words, an integrated process is a difference-stationary process, that is a process with a stochastic trends (see the previous chapter). I_1 &lt;- arima.sim(n = 500, list(order = c(0,1,0))) plot(I_1, main = &quot;I(1)&quot;) plot(diff(I_1), main = &quot;I(1) after &#39;differencing&#39;&quot;) 8.2.4 Seasonal (S) models We already introduced the seasonal model. For instance, a dataset showing a seasonal component is AirPassengers. The seasonality appears in the yearly fluctuations in the ACF and in the spikes occurring at 12 months from each other in the PACF. data(&quot;AirPassengers&quot;) # this function par(mfrow=c(..., ...)) # is used to combine more than one plot # in the same frame. The two numerical values # indicates number of rows and columns # the frame is made of par(mfrow=c(1,2)) acf(AirPassengers, lag.max = 48) pacf(AirPassengers, lag.max = 48) 8.2.5 Fit (S)ARIMA models The above examples represent simple processes, but real time series are often the result of more complex mixtures of different types of process, and therefore it is more complex to identify an appropriate model for the data. set.seed(7623) arima_112 &lt;- arima.sim(n = 500, list(order = c(1,1,2), ar = 0.8, ma = c(0.7, 0.2))) plot(arima_112, main = &quot;ARIMA(1,1,2)&quot;) A popular methods to find the appropriate model is the Box-Jenkins method, a recursive process involving the analysis of a time series, the guess of possible (S)ARIMA models, the fit of the hypothesized models, and a meta-analysis to determine the best specification. Once a best-fitting model has been found, the correlogram of the residuals should be verified as white noise. The Box-Jenkins method could be time-consuming and requires some expertise. ACF/PACF can also become difficult to read in case of complex models, and their appropriate interpretation could require a lot of expertise as well. Fortunately, experts have developed automated methods that allow us to automatically found and fit an ARIMA model. This is the case of the auto.arima function implemented in the forecast package (a package for time series analysis and especially for forecasting, developed by Rob J. Hyndman, professor of statistics and time series analysis expert). # Install the package if you haven&#39;t installed it yet # install.packages(&quot;forecast&quot;) library(forecast) arima_fit &lt;- auto.arima(arima_112) arima_fit ## Series: arima_112 ## ARIMA(1,1,2) ## ## Coefficients: ## ar1 ma1 ma2 ## 0.7649 0.7387 0.2484 ## s.e. 0.0356 0.0528 0.0534 ## ## sigma^2 estimated as 1.035: log likelihood=-717.9 ## AIC=1443.8 AICc=1443.88 BIC=1460.65 As said above, to evaluate the fit of a model we should analyze the residuals, and ascertain they behave as white noise. The object resulting from the function auto.arima has a slot including the residuals. To ascertain that residuals are white noise we can plot its ACF and PACF (no spike should be significant) and also its histogram. layout(matrix(c(1,2,3,3), nrow = 2)) acf(arima_fit$residuals) pacf(arima_fit$residuals) hist(arima_fit$residuals, main = &quot;Histogram of residuals&quot;) The forecast package also implements the function checkresiduals to create nice and complete plots of residual diagnostics by using a simple function. Besides creating the plots, the function calulate the Ljung-Box test (default), or the Breusch-Godfrey test (if you specify test=“BG” inside the function): The Ljung-Box test (and also the Breusch–Godfrey test) is a diagnostic tool, applied to the residuals of a time series after fitting an ARIMA model, to test the lack of fit. The test examines the autocorrelations of the residuals. If there are no significant autocorrelations, it can be concluded that the model does not exhibit significant lack of fit. To pass the test, the p-value has to be above the significance level (usually 0.05) checkresiduals(arima_fit) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,2) ## Q* = 5.8977, df = 7, p-value = 0.5517 ## ## Model df: 3. Total lags used: 10 8.2.5.1 SARIMA If we fit a model to the AirPassengers dataset, which has a seasonal component, we find a Seasonal Autoregressive Integrated Moving Average model (SARIMA). The seasonal component of the AirPassenger dataset is evident in the plot of the series and its ACF and PACF. The forecast package has a useful function ggtsdisplay to plot a time series along with its ACF and PACF. ggtsdisplay(AirPassengers) The seasonal part of the ARIMA model consists of terms that are similar to the non-seasonal components of the model, but involves lagged values of the seasonal period. AirPassengers_sarima &lt;- auto.arima(window(AirPassengers)) AirPassengers_sarima ## Series: window(AirPassengers) ## ARIMA(2,1,1)(0,1,0)[12] ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2143 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 estimated as 132.3: log likelihood=-504.92 ## AIC=1017.85 AICc=1018.17 BIC=1029.35 8.2.5.2 Forecasting Based on the ARIMA models we found, we can also try to forecast future values. We can use the function forecast of the homonym library. For instance, we could try to forecast the values of the AirPassenger dataset in the next four years. AirPassengers_forecast &lt;- forecast(AirPassengers_sarima, h=48, level = 90) plot(AirPassengers_forecast, main = &quot;AirPassengers forecast&quot;) AirPassengers_sarima &lt;- auto.arima(AirPassengers) 8.3 Cross-correlation Cross-correlation is the correlation between the (lagged) values of a time series and the values of another series. Similarly to ACF and PACF, there is a specific plot that shows the cross-correlation between two time series, and a specific R function: ccf. The cross-correlation can be useful to understand wich lagged values of a X series can be used to predict the values of a Y series, and thus used, for instance, in a time series regression model. Unfourtunately, the problem with the cross-correlation function is that, as we said in the preceding sections, with autocorrelated data it is difficult to assess the dependence between two processes, and it is possible to find spurious correlations. Thus, it is pertinent to disentangle the linear association between X and Y from their autocorrelation. A useful device for doing this is prewhitening. The prewhitening method works as follows: determine an ARIMA time series model for the X-variable, and store the residuals from this model; fit the ARIMA X-model to the Y-variable, and keep the residuals; examines the CCF between the X and Y model residuals. We can implement this procedure writing all the necessary code, or by using the library forecast, or also, alternatively, the library TSA. To make an example, we apply the method to two simulated two series. The Y-variable is created in such a way that it is correlated with the lagged values at time \\(x_{t-3}\\) and \\(x_{t-4}\\). Therefore, we should find a correlation at those lags. x_series &lt;- arima.sim(n = 200, list(order = c(1,1,0), ar = 0.7, sd=1)) z &lt;- ts.intersect(x_series, stats::lag(x_series, -3), stats::lag(x_series, -4)) y_series &lt;- 15 + 0.8*z[,2] + 1.5*z[,3] + rnorm(197,0,1) The cross-correlation applied to the original series results in a plot where everything seems to be correlated. The “real” correlation at lags \\(x_{t-3}\\) and \\(x_{t-4}\\) is not discernible at all. ccf(x_series, y_series, na.action = na.omit) By using the forecast library, we can calculate the pre-withened ccf as follows: # fit an ARIMA model x_model &lt;- auto.arima(x_series) # keep the residuals (&quot;white noise&quot;) x_residuals &lt;- x_model$residuals # fit the same ARIMA model to the Y-series # by using the &quot;Arima&quot; function in forecast y_model &lt;- Arima(y_series, model=x_model) # keep the residuals y_filtered &lt;- residuals(y_model) # apply the ccf to the residuals ccf(x_residuals, y_filtered) Now, it’s clear that the X-variable is correlated with the Y-variable at lags \\(x_{t-3}\\) and \\(x_{t-4}\\). The previous steps show in some detail the steps involved in the pre-whitening strategy, but it is possible to use the original series with the prewhiten function of the TSA library. Although the function can take, as an argument, a pre-fitted ARIMA model, its greater advantage is that it can take care of all the necessary steps to prewithen the series. In particular, if no model is specified, the library automatically applies a simple AR model. Although this model can be just an approximation of the “true” model (which can be more complex), an approximation can be enough to pre-whiten the series and find a proper cross-correlation (that is, also a simpler and approximate model can do the job). # install.packages(&quot;TSA&quot;) library(TSA) ## Registered S3 methods overwritten by &#39;TSA&#39;: ## method from ## fitted.Arima forecast ## plot.Arima forecast ## ## Attaching package: &#39;TSA&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## spec ## The following objects are masked from &#39;package:stats&#39;: ## ## acf, arima ## The following object is masked from &#39;package:utils&#39;: ## ## tar prewhiten(x_series, y_series) 8.4 Examples in literature A few examples to exemplify the use of ARIMA and Cross-Correlation in the scientific literature, with specific reference to communication science. In Scheufele, B., Haas, A., &amp; Brosius, H. B. (2011). Mirror or molder? A study of media coverage, stock prices, and trading volumes in Germany. Journal of Communication, 61(1), 48-70, the authors investigate “the short-term relationship between media coverage, stock prices, and trading volumes of eight listed German companies”, by using ARIMA and cross-correlation, in particular asking: RQ2: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the amount and the valence of coverage? RQ3: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the type of media (Financial Web sites, daily newspapers, and stock market TV shows) which reports on the company or stock? To answer these questions, the authors made use of time series analysis. In particular, they: estimated cross-lagged correlations between media coverage and stock prices or trading volumes, respectively. Basically, two steps of time-series analysis can be distinguished: (a) In the first step, each media time-series and each time-series of trading volumes was adjusted by ARIMA (Autoregressive Integrated Moving Average) modeling separately. The differences between the original time-series and its ARIMA model are called residuals and were used for analysis. Like with ordinary least squares regression, these residuals should not be auto-correlated. If the residuals are not auto-correlated, time-series analysis speaks of White Noise. This modeling technique called prewhitening was necessary to avoid spurious correlations. (…) (b) In the next step, cross-correlations between each adjusted media time-series and each adjusted stock series were calculated. (…) The coefficient expresses the strength of correlation, whereas the lags offer an insight into dynamics: Correlations at positive (negative) lags indicate that changes in media coverage proceeded (succeeded) shifts in stock prices or trading volumes. In Groshek, J. (2010). A time-series, multinational analysis of democratic forecasts and Internet diffusion. International Journal of Communication, 4, 33, the author examines the democratic effects that the Internet has shown using macro- level, cross-national data in a sequence of time–series statistical tests: this study relies principally on macro-level time–series democracy data from an historical sample that includes 72 countries, reaching back as far as 1946 in some cases, but at least from 1954 to 2003. From this sample, a sequence of ARIMA (autoregressive integrated moving average) time–series regressions were modeled for each country for at least 40 years prior to 1994. These models were then used to generate statistically-forecasted democracy values for each country, in each year from 1994 to 2003. A 95% confidence interval with an upper and lower democracy score was then constructed around each of the forecasted values using dynamic mean squared errors. The actual democracy scores of each country for each year from 1994 to 2003 were then compared to the upper and lower values of the confidence interval. In the event that the actual democracy level of any country was greater than the upper value of the forecasted democracy score during the time period of 1994 to 2003, Internet diffusion was investigated in case studies as a possible causal mechanism. In other terms, the author used a forecasting approach to predict the values of the series from 1994 to 2003, in order to find statistically significant differences between the predicted and the actual values. These discrepancies were interpreted as caused by factors that were not present in the past, and possibly by the introduction of the Internet. The study found that, based on the results of the 72 countries reported here, the diffusion of the Internet should not be considered a democratic panacea, but rather a component of contemporary democratization processes "],["regression.html", "Chapter 9 Regression 9.1 Static and Dynamic Models 9.2 Regression models 9.3 Regression with ARIMA errors 9.4 Some examples in the literature", " Chapter 9 Regression In this chapter we are going to see how to conduct a regression analysis with time series data. Regression analysis is a used for estimating the relationships between a dependent variable (DV) (also called outcome or response) and one or more independent variables (IV) (also called predictors or explanatory variables). A standard regression model \\(Y\\) = \\(\\beta\\) + \\(\\beta x\\) + \\(\\epsilon\\) has no time component. Differently, a time series regression model includes a time dimension and can be written, in a simple and general formulation, using just one explanatory variable, as follows: \\[ y_t = \\beta_0 + \\beta_1x_t + \\epsilon_t \\] In this equation, \\(y_t\\) is the time series we try to understand/predict (the dependent variable (DV)), \\(\\beta_0\\) is the intercept (a constant value that represents the expected mean value of \\(y_t\\) when \\(x_t = 0\\)), the coefficient \\(\\beta_1\\) is the slope, representing the average change in \\(y\\) at one unit increase in \\(x\\) (the independent variable (IV) or explanatory variable), and \\(\\epsilon_t\\) is the time series of residuals (the error term). A multiple regression, with more than one explanatory variable, can be written as follows: \\[ y_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t \\] 9.1 Static and Dynamic Models From a time series analysis perspective, a general distinction can be made between “static” and “dynamic” regression models: A static regression model includes just contemporary relations between the explanatory variables (independent variables) and the response (dependent variable). This model could be appropriate when the expected value of the response changes immediately when the value of the explanatory variable changes. Considering a model with \\(k\\) independent variables {\\(x_1\\), \\(x_2\\), …, \\(x_k\\)}, a static (multiple) regression model, has the form just seen above: \\[ y_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t \\] Each \\(\\beta\\) coefficient models the instant change in the conditional expected value of the response variable \\(y_t\\) as the value of \\(x_{k,t}\\) changes by one unit, keeping constant all the other predictors (i.e.: the other \\(x_{k,t}\\)): A dynamic regression model includes relations between both the current and the lagged (past) values of the explanatory (independent) variables, that is, the expected value of the response variable may change after a change in the values of the explanatory variables. \\[ \\begin{aligned} y_t = \\beta_0 &amp; + \\beta_{10}x_{1,t} + \\beta_{11}x_{1,t-1} + ... + \\beta_{1m}x_{1,t-m} \\\\ &amp; + \\beta_{20}x_{2,t} + \\beta_{21}x_{2,t-1} + ... + \\beta_{2m}x_{2,t-m} \\\\ &amp; + \\dots \\\\ &amp; + \\beta_{k0}x_{k,t} + \\beta_{k1}x_{k,t-1} + ... + \\beta_{km}x_{2,t-m} \\\\ &amp; + \\epsilon_t \\\\ \\end{aligned} \\] Despite the differences between these two analytic perspectives, the term dynamic regression is also used, in the literature, in a more general way to refer to regression models with autocorrelated errors (also when they are used to analyze only contemporary relations between variables). 9.2 Regression models Except for the possible use of lagged regressors, which are typical of time series, the above described statistical models are standard regression models, commonly used with cross-sectional data. Standard linear regression models can sometimes work well enough with time series data, if specific conditions are met. Besides standard assumptions of linear regression1, a careful analysis should be done in order to ascertain that residuals are not autocorrelated, since this can cause problems in the estimated model. Even before that, it is important that the series are stationary, in order to avoid possible spurious correlations. Since time series can be nonstationary due to different reasons, different strategies can be employed to stationarize the data. For instance, a nonstationary series can be a series with unequal variance over time. A common way to try to fix the problem is applying the log-transformation. library(xts) elections_news &lt;- read.csv(&quot;data/elections-stories-over-time-20210111144254.csv&quot;) elections_news$date &lt;- as.Date(elections_news$date) elections_news &lt;- xts(elections_news$count, order.by = elections_news$date) elections_news_log &lt;- log(elections_news+1) elections_news_xts &lt;- merge.xts(elections_news, elections_news_log) plot.xts(elections_news_xts, col = c(&quot;blue&quot;, &quot;red&quot;), multi.panel = TRUE, yaxis.same = FALSE, main = &quot;Original vs Log-transformed series&quot;) Another reason for nonstationarity is the periodic variation due to seasonality (regular fluctuations in a time series that follow a specific time pattern, e.g.: social media activity during week-ends, Christmas effect in consumption, etc.). To remove the seasonal pattern, you might want to use a seasonally-adjusted time series. Otherwise, you could create a dummy variable for the seasonal period (that is, a variable that follows the seasonal pattern in the data in order to account, in the model, for these fluctuations). # load the ts dataset AirPassenger data(&quot;AirPassengers&quot;) # remove seasonality from a multiplicative model AirPassengers_decomposed &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) AirPassengers_seasonal_component &lt;- AirPassengers_decomposed$seasonal AirPassengers_seasonally_adjusted &lt;- AirPassengers/AirPassengers_seasonal_component par(mfrow=c(1,2)) plot.ts(AirPassengers, col = &quot;blue&quot;, main = &quot;Original series&quot;) plot.ts(AirPassengers_seasonally_adjusted, col = &quot;blue&quot;, main = &quot;Seasonally-adjusted series&quot;, ylab = &quot;Seasonally-adjusted values&quot;) An important reason for nonstationarity is also the presence of a trend in the data. There are stochastic and deterministic trends. Deterministic trends are a fixed function of time, while stochastic trends change in an unpredictable way. Series with a deterministic trend are also called trend stationary because they can be stationary around a deterministic trend, and it could be possible to achieve stationarity by removing the time trend. In trend stationary processes, the shocks to the process are transitory and the process is mean reverting. Processes with a stochastic trend are also called difference stationary because they can become stationary through differencing. In series with stochastic trends we could see that shocks have permanent effects. When dealing with deterministic trend, we might want to work with detrended series. # remove the trend from a multiplicative model AirPassengers_decomposed &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) AirPassengers_trend_component &lt;- AirPassengers_decomposed$trend AirPassengers_detrended &lt;- AirPassengers/AirPassengers_trend_component par(mfrow=c(1,2)) plot.ts(AirPassengers, col = &quot;blue&quot;, main = &quot;Original series&quot;) plot.ts(AirPassengers_detrended, col = &quot;blue&quot;, main = &quot;Detrended series&quot;, ylab = &quot;Detrended values&quot;) Otherwise, in regression analysis, it is also commonly add a dummy variable consisting of a value that increases with time, to account for a linear deterministic time trend. This time-count variable will remove the deterministic trend from the dependent variable, allowing the other predictors to explain the remaining variance. # create a simulate series set.seed(1312) toy_data &lt;- arima.sim(n = 100, model = list(order = c(0,0,0))) # add a deterministic trend to the series toy_data_trend &lt;- toy_data + 0.2*1:length(toy_data) par(mfrow=c(1,3)) plot.ts(toy_data, main = &quot;Original series&quot;) plot.ts(toy_data_trend, main = &quot;Series with Trend&quot;) dummy_trend &lt;- 1:length(toy_data_trend) lm_toydata &lt;- lm(toy_data_trend ~ dummy_trend) plot.ts(lm_toydata$residuals, main = &quot;Residuals (detrended)&quot;) When we have a series with a stochastic trend, we can achieve stationarity through differencing. set.seed(111) Random_Walk &lt;- arima.sim(n = 500, model = list(order = c(0,1,0))) Random_Walk_diff &lt;- diff(Random_Walk) par(mfrow=c(1,2)) plot.ts(Random_Walk, main = &quot;Random Walk&quot;, col = &quot;blue&quot;, ylab=&quot;&quot;) plot.ts(Random_Walk_diff, main = &quot;Differenced Random Walk&quot;, col = &quot;blue&quot;, ylab=&quot;&quot;) There are tests for detecting different types of trends. We will learn more about them in the next lecture. 9.2.1 Non-autocorrelated residuals We try to fit a linear regression model. First, we create two series \\(x\\) and \\(y\\), with \\(x\\) correlated with \\(y\\) at lags \\(x_{t-3}\\) and \\(x_{t-4}\\). # simulated data # the x series is correlated at lag 3 and 4 set.seed(999) x_series &lt;- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1)) z &lt;- ts.intersect(x_series, stats::lag(x_series, -3), stats::lag(x_series, -4)) y_series &lt;- 15 + 0.8*z[,2] + 1.5*z[,3] + rnorm(196,0,1) xLagged &lt;- cbind( xLag0 = x_series, xLag3 = stats::lag(x_series,-3), xLag4 = stats::lag(x_series,-4)) xy_series &lt;- ts.union(y_series, xLagged) The real model (in this case we know it because we created it through the above simulation), is as follows: \\[ y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1) \\] To fit a linear regression, we use the function lm (it’s in base R, no additional packages are necessary). lm1 &lt;- lm(xy_series[,1] ~ xy_series[,3:4]) The function summary is used to print the summary of the model, which includes the estimates (the “coefficients” of the variables) and other important information. summary(lm1) ## ## Call: ## lm(formula = xy_series[, 1] ~ xy_series[, 3:4]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.56906 -0.72401 0.05896 0.74288 2.14534 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.96732 0.07299 205.06 &lt;2e-16 *** ## xy_series[, 3:4]xLagged.xLag3 0.86046 0.07753 11.10 &lt;2e-16 *** ## xy_series[, 3:4]xLagged.xLag4 1.41678 0.07831 18.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.007 on 193 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.8698, Adjusted R-squared: 0.8684 ## F-statistic: 644.5 on 2 and 193 DF, p-value: &lt; 2.2e-16 We said that regression models sometimes work well enough with time series data, if specific conditions are met. Regards the conditions (or assumptions), in particular, the residuals of the models should have zero mean, they shouldn’t show any significant autocorrelation, and they should be normally distributed. To check whether these assumptions are met, we can visualize the plot of residuals, its ACF/PACF and histogram, and also test the residuals for possible autocorrelation using a statistical test like the Breusch-Godfrey test (this test is the default in the forecast library when a linear regression object lm is tested). To create the plots we can use the base R functions, or we can use the convenient checkresiduals function in the forecast package. In this case everything seems fine. # install.package(&quot;forecast&quot;) # install the package if necessary library(forecast) checkresiduals(lm1) ## ## Breusch-Godfrey test for serial correlation of order up to 10 ## ## data: Residuals ## LM test = 9.0522, df = 10, p-value = 0.5272 If we look at the model summary printed above, we can see that the estimated model is the following (the standard deviation of residuals is misnamed as “residual standard error” in the summary of lm): \\[ \\begin{equation} y_t = 14.96732 + 0.86046x_{t-3} + 1.41678x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1.007^2) \\end{equation} \\] The estimated model is also close to the “true” model: \\[ y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1) \\] ### Autocorrelated residuals While in the previous case a standard linear model works well, often the residuals of times series regressions are autocorrelated, and a linear regression model can be suboptimal or even wrong. For instance, let’s create other two time series that are, as the previous ones, cross-correlated at lag 3 and 4, but with a bit more complicated structure. # another set of simulated data # the x series is correlated at lag 3 and 4 set.seed(999) x2_series &lt;- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1)) z2 &lt;- ts.intersect(x2_series, stats::lag(x2_series, -3), stats::lag(x2_series, -4)) y2_series &lt;- 15 + 0.8*z2[,2] + 1.5*z2[,3] y2_errors &lt;- arima.sim(n = 196, list(order = c(1,0,1), ar = 0.6, ma = 0.6), sd=1) y2_series &lt;- y2_series + y2_errors x2Lagged &lt;- cbind( xLag0 = x2_series, xLag3 = stats::lag(x2_series,-3), xLag4 = stats::lag(x2_series,-4)) xy2_series &lt;- ts.union(y2_series, x2Lagged) # check the cross-correlations at lag 3 and 4 library(TSA) prewhiten(x2_series, y2_series) Considering the autocorrelated structure of the series, the true model can be written as follows: \\[ \\begin{aligned} &amp; y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\eta_t \\\\ &amp; \\eta_t = 0.7\\eta_{t-1} + \\epsilon_t + 0.6\\epsilon_{t-1} \\\\ &amp; \\epsilon \\sim N(0, 1) \\end{aligned} \\] lm2 &lt;- lm(xy2_series[,1] ~ xy2_series[,3:4]) summary(lm2) # AIC: 821.45 ## ## Call: ## lm(formula = xy2_series[, 1] ~ xy2_series[, 3:4]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9200 -1.4508 0.0667 1.5395 5.1083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.9005 0.1486 100.273 &lt; 2e-16 *** ## xy2_series[, 3:4]x2Lagged.xLag3 1.0407 0.1595 6.523 6.14e-10 *** ## xy2_series[, 3:4]x2Lagged.xLag4 1.5171 0.1599 9.488 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.028 on 189 degrees of freedom ## (12 observations deleted due to missingness) ## Multiple R-squared: 0.6735, Adjusted R-squared: 0.67 ## F-statistic: 194.9 on 2 and 189 DF, p-value: &lt; 2.2e-16 The estimated model is the following: \\[ \\begin{aligned} &amp; y_t = 14.9005 + 1.0407x_{t-3} + 1.5171x_{t-4} + \\epsilon_t \\\\ &amp; \\epsilon \\sim N(0, 2.028^2) \\end{aligned} \\] The original series can also be visualized with the fitted values (the values resulting from the model), to visually inspect how well the model represents the original series. The differences between the original and the fitted series are the residuals. lm2d &lt;- ts.intersect(na.omit(xy2_series[,1]), lm2$fitted.values) plot.ts(lm2d, plot.type = &quot;single&quot;, col=c(&quot;orange&quot;,&quot;blue&quot;), lty=c(1,4), lwd=c(1,1), main = &quot;&#39;Classic&#39; Linear Model - Original (orange) and Fitted series (blue)&quot;) The diagnostic plots of the residuals show the presence of autocorrelation, and the Breusch-Godfrey test is highly significant (its value is far lower than the critical value \\(\\alpha = 0.05\\)) checkresiduals(lm2) ## ## Breusch-Godfrey test for serial correlation of order up to 10 ## ## data: Residuals ## LM test = 149.45, df = 10, p-value &lt; 2.2e-16 pacf(lm2$residuals) In this case, it’s better to take into account the residuals’ autocorrelation by using a regression model capable to handle autocorrelated time series structures. 9.3 Regression with ARIMA errors In the previous chapter we said that ARIMA models are a special type of regression model, in which the dependent variable is the time series itself, and the independent variables are all lags of the time series. This model is capable to take into account the autocorrelated structure of time series. ARIMA is a modeling technique that can be applied to a single time series, but it can be extended to include additional, exogenous variables. The ARIMA model including exogenous regressors (i.e.: other time series besides the lagged dependent variable) is like a multiple regression models for time series. In particular, it can be considered a regression model capable to control for autocorrelation in residuals. It is possible to use more than one option to fit an ARIMA model with external regressors. A convenient option is provided by the function auto.arima, in the package forecast. This library has an argument xreg which can be use with a numerical vector or matrix of external regressors, which must have the same number of rows as y (see ?auto.arima). set.seed(999) arima1 &lt;- auto.arima(xy2_series[,1], xreg = xy2_series[,3:4]) arima1 ## Series: xy2_series[, 1] ## Regression with ARIMA(1,0,1) errors ## ## Coefficients: ## ar1 ma1 intercept x2Lagged.xLag3 x2Lagged.xLag4 ## 0.6863 0.6491 14.8532 0.9506 1.5732 ## s.e. 0.0555 0.0528 0.3607 0.0757 0.0762 ## ## sigma^2 estimated as 0.9482: log likelihood=-265.76 ## AIC=543.52 AICc=543.97 BIC=563.06 The resulting model seems to be more appropriate than the previous one, fitted by using just a “classic” linear regression. This is clear also by comparing the two models through the AIC criterium (Akaike information criterion). The AIC value is used to compare the goodness-of-fit of different models fitted to the same dataset. The lower the AIC value, the better the fit. The auto.arima function prints the AIC value by default, while this value is not given with the lm function. To get it, we need to use the AIC function. AIC(lm2) ## [1] 821.4495 In this case, the ARIMA regression model results a far better model (AIC=543.52) compared with the classic linear model (AIC=821.45). \\[ \\begin{aligned} &amp; y_t = 14.8532 + 0.9506x_{t-3} + 1.5732x_{t-4} + \\eta_t \\\\ &amp; \\eta_t = 0.6863\\eta_{t-1} + \\epsilon_t + 0.6491\\epsilon_{t-1} \\\\ &amp; \\epsilon \\sim N(0, 0.9482) \\end{aligned} \\] Diagnostic analysis of the residuals, shows that there is no concerning sign of autocorrelation in the residuals, which looks like white noise. Also the test for autocorrelated errors is not significant (the default test for autocorrelation when testing an ARIMA models with external regressors in the forecast package is the Ljung-Box test)2). checkresiduals(arima1) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,1) errors ## Q* = 6.3861, df = 5, p-value = 0.2704 ## ## Model df: 5. Total lags used: 10 Also by visually inspect the original series along with the fitted series (the values resulting from the model), it can be seen that the model is better than the previous one. arima1d &lt;- ts.intersect(na.omit(xy2_series[,1]), arima1$fitted) plot.ts(arima1d, plot.type = &quot;single&quot;, col=c(&quot;orange&quot;,&quot;blue&quot;), lty=c(1,4), lwd=c(1,1), main = &quot;ARIMA errors model - Original (orange) and Fitted series (blue)&quot;) We can also compare the fitted versus original values by using a scatterplot. A better model produces a thinner diagonal line. par(mfrow=c(1,2)) plot(na.omit(xy2_series[,1]), lm2$fitted.values, main = &quot;LM&quot;, xlab=&quot;Original&quot;, ylab=&quot;Fitted&quot;) plot(na.omit(xy2_series[,1]), arima1$fitted, main = &quot;ARIMA regression model&quot;, xlab=&quot;Original&quot;, ylab=&quot;Fitted&quot;) 9.4 Some examples in the literature There are several examples of the use of time series regression models in the literature in the field of communication science. For instance, in The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates3, the authors examined whether the UN climate change conferences are conducive to an emergence of a transnational public sphere by triggering issue convergence and increased transnational interconnectedness across national media debates. They authors detail the method they follows in this way: […] Given the autoregressive nature and other properties of time series, an ordinary least squares regression analysis would violate the normality of error and the independence of observations assumption (Wells et al., 2019). Instead, we applied the dynamic regression approach (Gujarati &amp; Porter, 2009; Hyndman &amp; Athanasopoulos, 2018), which assumes that the error term follows an autoregressive integrated moving average (ARIMA) model (…). we found the best ARIMA structure of the error term by using the auto.arima function from the forecast R package (Hyndman &amp; Khandakar, 2008). It searches for an ARIMA structure that can explain the most variance according to the Akaike information criterion (Akaike, 1973). In this case they use the term “dynamic regression” to refer to a time series regression with ARIMA errors, but they did not include lagged values of their variables, thus analyzing contemporary relationships between variables. The found, for instance, that events taking place on a supranational level of governance (…) consistently led to spikes in media attention across countries. In contrast, a bottom-up effort such as Fridays for Future showed an inconsistent relationship with media attention across the four countries. In Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event4, the authors used both standard regression and regression with ARIMA errors to show that “online incivility — operationalized as the use of foul language — grew as volume of political discussions and levels of cyberbalkanization increased. Incivility led to higher levels of opinion polarization.”. Also in this case the authors analyze a “static process”, that is, focus on contemporary relationships between variables. In Beyond cognitions: A longitudinal study of online search salience and media coverage of the president5, the authors used regression models with ARIMA errors to examine shifts in newswire coverage and search interest among Internet users in President Obama during the first two years of his administration (2009-2010). In this case, the authors analyze relationships between variables taking into account lagged values, thus adopting a “dynamic process” perspective. For instance, they write: RQ2 sought to determine the time span of linkages between coverage volume and search volume. (…) ARIMA models were run to gauge the dynamics of mutual influence between these two time series. The first model examined the effect of coverage volume on search volume over time (i.e., basic agenda setting) (…) presidential public relations, was included as an additional input series. The first model, with search volume being a single dependent variable, was identified through a close examination of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs). This analysis revealed a classic autoregressive model for the series (1, 0, 0). […] According to the results, shifts in aggregate search volume over this two-year period were significantly predicted by coverage volume over the prior five weeks (p &lt; .010)* and by presidential public relations efforts in the preceding two, three (p &lt; .001), and five weeks (p &lt; .005). The ARIMA model with two predictors was correctly specified (Ljung–Box Q = 18.132, p = .381) and it explained roughly 35% of the observed variation in the series. In AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–20076, the authors examined the effect of newspaper coverage of HIV/AIDS on HIV testing behavior in a U.S. population., using a lagged regression to support causal order claims by ensuring that newspaper coverage precedes the testing behavior with the inclusion of the 1-month lagged newspaper coverage variable in the model. Counterintuitively, they found that the news media coverage had a negative effect on testing behavior: For every additional 100 HIV/AIDS risk related newspaper stories published in this group of U.S. newspapers each month, there was a 1.7% decline in HIV testing levels in the following month, with a higher negative effects on African Americans. 1) Linearity: The relationship between X and Y must be linear; 2) Independence of errors: There is not a relationship between the residuals and the Y variable; 3) Normality of errors: The residuals must be approximately normally distributed; 4) Equal variances: The variance of the residuals is the same for all values of X↩︎ There are many tests for detecting autocorrelation. Besides the already mentioned Breusch-Godfrey test and Ljung-Box test, other popular tests are the Durbin Watson test, and the Box–Pierce test. Each test has its own characteristics. For instance, the Durbin-Watson test is a popular way to test for autocorrelation, but it shouldn’t be used with lagged dependent variables. In this case it can be used the Breusch-Godfrey test↩︎ Wozniak, A., Wessler, H., Chan, C. H., &amp; Lück, J. (2021). The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates. International Journal of Communication, 15(27)↩︎ Lee, F. L., Liang, H., &amp; Tang, G. K. (2019). Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event. International Journal of Communication, 13, 20.↩︎ Ragas, M. W., &amp; Tran, H. (2013). Beyond cognitions: A longitudinal study of online search salience and media coverage of the president. Journalism &amp; Mass Communication Quarterly, 90(3), 478-499.↩︎ Stevens, R., &amp; Hornik, R. C. (2014). AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007. Journal of health communication, 19(8), 893-906↩︎ "],["intervention-analysis.html", "Chapter 10 Intervention Analysis", " Chapter 10 Intervention Analysis In this chapter we are going to see how to conduct a intervention analysis. Intervention analysis is a method introduced by Box and Tiao (1975)7, which provides a framework for assessing the effect of an intervention on a time series under study. As summarized by Box and Tiao: Given a known intervention, is there evidence that change in the series of the kind expected actually occurred, and, if so, what can be said of the nature and magnitude of the change?. Intervention analysis is a “quasi-experimental” design and an interesting approach to test whether exogenous shocks, such as, for instance, the introduction of a new policy, impact on a time series process in a significant way, that is, by changing the mean function or trend of a time series. Interventions can have different impacts. For instance, an intervention can have an abrupt impact determining a permanent or temporary change, a sudden and short-lived change due to an event, or a more gradual yet permanent change. To conduct such an analysis, it is necessary to know, at least, the date of the intervention. Regarding the statistical approach, different approaches are possible. ARIMA modeling is a classic choice, and we can come back to it later. Instead, we start from a more complex Bayesian approach implemented in the convenient package CausalImpact developed and used at Google to estimate causal impacts in a quasi-experimental framework8. # Install and load the package # install.packages(&quot;CausalImpact&quot;) library(CausalImpact) ## Loading required package: bsts ## Loading required package: BoomSpikeSlab ## Loading required package: Boom ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## ## Attaching package: &#39;Boom&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## rWishart ## ## Attaching package: &#39;BoomSpikeSlab&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## knots ## ## Attaching package: &#39;bsts&#39; ## The following object is masked from &#39;package:BoomSpikeSlab&#39;: ## ## SuggestBurn We use the simulated dataset used by the Google tutorial on the package, creating two time series \\(y\\) and \\(x\\) of length 100, simulating an abrupt intervention at time 71 determining a permanent increment of 10 points in the \\(y\\) series. set.seed(1) x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100) y &lt;- 1.2 * x1 + rnorm(100) y[71:100] &lt;- y[71:100] + 10 dat &lt;- ts.intersect(y, x1) Then, it is necessary to specify the pre-intervention and post-intervention period. In the pre-intervention period no impact is expected. pre.period &lt;- c(1, 70) post.period &lt;- c(71, 100) The function CausalImpact uses the values of the original time series \\(y\\) in the pre-intervention period, and the predictors correlated to the \\(y\\) (in this case \\(x\\)), to forecast the values that \\(y\\) would have had without the intervention (counterfactual). To accurately forecast the \\(y\\) values, which is necessary to obtain valid results from the analysis, it is necessary to have a proper model of the \\(y\\) series (based on the series itself and its predictors). Then, the differences in the expected (forecasted) \\(y\\) values without intervention, and the actual \\(y\\) values following the intervention, are compared in order to estimate the impact of the intervention. impact &lt;- CausalImpact(dat, pre.period, post.period) By using the function plot on the resulting model, three plots are visualized: The first panel shows the data and a counterfactual prediction for the post-treatment period. The second panel shows the difference between observed data and counterfactual predictions. This is the pointwise causal effect, as estimated by the model. The third panel adds up the pointwise contributions from the second panel, resulting in a plot of the cumulative effect of the intervention. (…) the above inferences depend critically on the assumption that the covariates were not themselves affected by the intervention. The model also assumes that the relationship between covariates and treated time series, as established during the pre-period, remains stable throughout the post-period. plot(impact) Besides plotting the results, it is possible to create a summary of the model, and by adding the argument “report” inside the function summary, a convenient explanations of the results is printed. summary(impact) ## Posterior inference {CausalImpact} ## ## Average Cumulative ## Actual 117 3511 ## Prediction (s.d.) 107 (0.37) 3196 (10.96) ## 95% CI [106, 107] [3175, 3217] ## ## Absolute effect (s.d.) 11 (0.37) 315 (10.96) ## 95% CI [9.8, 11] [294.0, 336] ## ## Relative effect (s.d.) 9.9% (0.34%) 9.9% (0.34%) ## 95% CI [9.2%, 11%] [9.2%, 11%] ## ## Posterior tail-area probability p: 0.00101 ## Posterior prob. of a causal effect: 99.8993% ## ## For more details, type: summary(impact, &quot;report&quot;) summary(impact, &quot;report&quot;) ## Analysis report {CausalImpact} ## ## ## During the post-intervention period, the response variable had an average value of approx. 117.05. By contrast, in the absence of an intervention, we would have expected an average response of 106.54. The 95% interval of this counterfactual prediction is [105.83, 107.25]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 10.51 with a 95% interval of [9.80, 11.21]. For a discussion of the significance of this effect, see below. ## ## Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 3.51K. By contrast, had the intervention not taken place, we would have expected a sum of 3.20K. The 95% interval of this prediction is [3.18K, 3.22K]. ## ## The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +10%. The 95% interval of this percentage is [+9%, +11%]. ## ## This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (10.51) to the original goal of the underlying intervention. ## ## The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant. The authors of the package underline the importance of the statistical assumptions to get valid results, and about possible strategies to ascertain that the assumptions are met, they write the following advice: Here are a few ways of getting started. First of all, it is critical to reason why the covariates that are included in the model (this was x1 in the example) were not themselves affected by the intervention. Sometimes it helps to plot all covariates and do a visual sanity check. Next, it is a good idea to examine how well the outcome data y can be predicted before the beginning of the intervention. This can be done by running CausalImpact() on an imaginary intervention. Then check how well the model predicted the data following this imaginary intervention. We would expect not to find a significant effect, i.e., counterfactual estimates and actual data should agree reasonably closely. Finally, when presenting or writing up results, be sure to list the above assumptions explicitly, including the priors in model.args, and discuss them with your audience. Box, G. E., &amp; Tiao, G. C. (1975). Intervention analysis with applications to economic and environmental problems. Journal of the American Statistical association, 70(349), 70-79↩︎ CausalImpact 1.2.1, Brodersen et al., Annals of Applied Statistics (2015). http://google.github.io/CausalImpact/↩︎ "],["readings.html", "Chapter 11 Readings 11.1 Bibliographical References", " Chapter 11 Readings This page will be updated as the course goes on. Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., &amp; Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13. 11.1 Bibliographical References Gaubatz, K. T. (2014). A Survivor’s Guide to R: An Introduction for the Uninitiated and the Unnerved. SAGE Publications. "]]
